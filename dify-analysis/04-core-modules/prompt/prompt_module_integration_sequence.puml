@startuml Prompt_Module_Integration
!theme plain
skinparam backgroundColor #FFFFFF
skinparam sequenceArrowColor #1976D2
skinparam sequenceActorBorderColor #426450ff
skinparam sequenceParticipantBorderColor #6C757D
skinparam SequenceParticipantBorderThickness 2
skinparam sequenceLifeLineBorderColor #9bd0f5ff
skinparam noteBackgroundColor #FFF3E0
skinparam noteBorderColor #F57C00
skinparam style strictuml
skinparam Padding      6
skinparam ParticipantPadding    30

title Prompt 模块与应用层集成时序图

actor "用户" as User
participant "API Controller" as API
participant "ChatAppRunner /\nCompletionAppRunner" as AppRunner
participant "BaseAppRunner" as BaseRunner
participant "Prompt Module" as Prompt
participant "TokenBufferMemory" as Memory
participant "ModelManager" as ModelMgr
participant "LLM Provider" as LLM

== 1. 用户发起请求 ==

User -> API: 发送聊天/补全请求
activate API

API -> AppRunner: run(app_generate_entity)
activate AppRunner

== 2. 获取上下文和历史 ==

AppRunner -> AppRunner: 获取应用配置
AppRunner -> AppRunner: 加载 prompt_template_entity

alt 需要 RAG 上下文
    AppRunner -> AppRunner: 执行知识库检索
    AppRunner -> AppRunner: context = 检索结果
end

alt 需要对话历史
    AppRunner -> Memory: 初始化 TokenBufferMemory
    activate Memory
    Memory --> AppRunner: memory 实例
    deactivate Memory
end

== 3. 构建 Prompt ==

AppRunner -> BaseRunner: organize_prompt_messages(\n  app_record,\n  model_config,\n  prompt_template_entity,\n  inputs,\n  files,\n  query,\n  context,\n  memory\n)
activate BaseRunner

BaseRunner -> BaseRunner: 判断 prompt_type

alt prompt_type == SIMPLE
    note right of BaseRunner
        Chatbot 基础模式
        使用预设模板
    end note
    
    BaseRunner -> Prompt: SimplePromptTransform.get_prompt(...)
    activate Prompt
    
    Prompt -> Prompt: 加载 JSON 模板规则
    Prompt -> Prompt: 解析变量
    Prompt -> Prompt: 构建系统消息
    
    alt memory 存在
        Prompt -> Memory: get_history_prompt_messages()
        activate Memory
        Memory --> Prompt: 历史消息列表
        deactivate Memory
        Prompt -> Prompt: 追加历史消息
    end
    
    Prompt -> Prompt: 添加用户消息
    
    Prompt --> BaseRunner: (prompt_messages, stops)
    deactivate Prompt

else prompt_type == ADVANCED
    note right of BaseRunner
        高级模式
        使用自定义模板
    end note
    
    BaseRunner -> BaseRunner: 构建 prompt_template
    BaseRunner -> BaseRunner: 构建 memory_config
    
    BaseRunner -> Prompt: AdvancedPromptTransform.get_prompt(...)
    activate Prompt
    
    Prompt -> Prompt: 遍历模板消息
    Prompt -> Prompt: 解析变量（Basic/Jinja2）
    Prompt -> Prompt: 设置上下文变量
    
    alt memory 存在
        Prompt -> Memory: 获取历史消息
        activate Memory
        Memory --> Prompt: 历史消息
        deactivate Memory
    end
    
    Prompt -> Prompt: 处理文件附件
    Prompt -> Prompt: 添加查询消息
    
    Prompt --> BaseRunner: prompt_messages
    deactivate Prompt
    
    BaseRunner -> BaseRunner: stops = model_config.stop
end

BaseRunner --> AppRunner: (prompt_messages, stops)
deactivate BaseRunner

== 4. 调用 LLM ==

AppRunner -> ModelMgr: 获取模型实例
activate ModelMgr
ModelMgr --> AppRunner: model_instance
deactivate ModelMgr

AppRunner -> AppRunner: recalc_llm_max_tokens()
note right: 根据 prompt 长度重新计算 max_tokens

AppRunner -> LLM: invoke_llm(prompt_messages, stops, ...)
activate LLM

alt stream == True
    loop 流式输出
        LLM --> AppRunner: chunk
        AppRunner --> API: 推送 chunk
        API --> User: SSE 事件
    end
else stream == False
    LLM --> AppRunner: result
end
deactivate LLM

== 5. 保存记录 ==

AppRunner -> AppRunner: 保存消息记录
note right
    使用 PromptMessageUtil
    序列化 prompt_messages
end note

AppRunner --> API: 完成响应
deactivate AppRunner

API --> User: 返回结果
deactivate API

@enduml
