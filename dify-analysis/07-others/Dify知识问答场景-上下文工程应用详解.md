# Dify çŸ¥è¯†é—®ç­”åœºæ™¯ - ä¼šè¯çº§ä¸Šä¸‹æ–‡å·¥ç¨‹åº”ç”¨è¯¦è§£

## âš ï¸ é‡è¦è¯´æ˜

**æœ¬æ–‡æ¡£è®¨è®ºçš„æ˜¯"ä¼šè¯çº§ä¸Šä¸‹æ–‡å·¥ç¨‹"ï¼ˆSession-Level Context Engineeringï¼‰**ï¼Œå³åœ¨**å•æ¬¡å¯¹è¯ä¼šè¯å†…**ç®¡ç†ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

å¦‚æœæ‚¨åœ¨å¯»æ‰¾"ç”¨æˆ·çº§ä¸Šä¸‹æ–‡å·¥ç¨‹"ï¼ˆè·¨ä¼šè¯çš„ç”¨æˆ·ç”»åƒã€ä¸ªæ€§åŒ–è®°å¿†ç­‰ï¼‰ï¼Œè¯·å‚é˜… [ä¸Šä¸‹æ–‡å·¥ç¨‹æ¦‚å¿µæ¾„æ¸….md](./ä¸Šä¸‹æ–‡å·¥ç¨‹æ¦‚å¿µæ¾„æ¸….md)ã€‚

---

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è§£æ Dify åœ¨çŸ¥è¯†é—®ç­”åœºæ™¯ä¸­å¦‚ä½•åº”ç”¨**ä¼šè¯çº§ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ˆSession-Level Context Engineeringï¼‰**æ¥ä¼˜åŒ–ç”¨æˆ·ä½“éªŒã€‚é€šè¿‡æ·±å…¥ä»£ç åˆ†æï¼Œæ­ç¤º Dify å¦‚ä½•é€šè¿‡å¤šå±‚æ¬¡çš„ä¸Šä¸‹æ–‡ç®¡ç†æŠ€æœ¯ï¼Œå®ç°å‡†ç¡®ã€è¿è´¯ã€å¯è¿½æº¯çš„æ™ºèƒ½é—®ç­”ã€‚

## ğŸ¯ ä»€ä¹ˆæ˜¯ä¼šè¯çº§ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼Ÿ

ä¼šè¯çº§ä¸Šä¸‹æ–‡å·¥ç¨‹æ˜¯æŒ‡åœ¨ LLM åº”ç”¨çš„**å•æ¬¡å¯¹è¯ä¼šè¯å†…**ï¼Œç³»ç»ŸåŒ–åœ°**æ”¶é›†ã€ç»„ç»‡ã€æ³¨å…¥å’Œç®¡ç†ä¸Šä¸‹æ–‡ä¿¡æ¯**çš„æŠ€æœ¯å®è·µã€‚

**èŒƒå›´ï¼š** ä»…åœ¨å½“å‰å¯¹è¯ä¼šè¯ä¸­æœ‰æ•ˆï¼Œä¼šè¯ç»“æŸåä¸Šä¸‹æ–‡ä¸ä¿ç•™

**æ ¸å¿ƒå†…å®¹ï¼š**
- ğŸ“š çŸ¥è¯†åº“æ£€ç´¢ç»“æœ (`{{#context#}}`)
- ğŸ’¬ å¯¹è¯å†å² (`{{#histories#}}`) - å½“å‰ä¼šè¯çš„å†å²æ¶ˆæ¯
- â“ ç”¨æˆ·æŸ¥è¯¢ (`{{#query#}}`) - å½“å‰é—®é¢˜

**ç›®çš„ï¼š**

1. **æé«˜å›ç­”å‡†ç¡®æ€§** - åŸºäºç›¸å…³çŸ¥è¯†è€Œéæ¨¡å‹å¹»è§‰
2. **ä¿æŒå¯¹è¯è¿è´¯æ€§** - è®°å¿†å’Œåˆ©ç”¨å†å²å¯¹è¯
3. **å¢å¼ºå¯æ§æ€§** - é€šè¿‡ä¸Šä¸‹æ–‡å¼•å¯¼æ¨¡å‹è¡Œä¸º
4. **ä¼˜åŒ– Token ä½¿ç”¨** - åŠ¨æ€è£å‰ªï¼Œé¿å…è¶…é™

---

## ğŸ—ï¸ Dify çš„ä¸Šä¸‹æ–‡å·¥ç¨‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    çŸ¥è¯†é—®ç­”åœºæ™¯æ¶æ„                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  [ç”¨æˆ·è¾“å…¥] â†’ [çŸ¥è¯†æ£€ç´¢] â†’ [ä¸Šä¸‹æ–‡æ„å»º] â†’ [Promptç»„è£…]        â”‚
â”‚                    â†“            â†“            â†“               â”‚
â”‚              å‘é‡æ£€ç´¢     æ–‡æ¡£è¿‡æ»¤     æ¨¡æ¿å˜é‡æ³¨å…¥            â”‚
â”‚              å…ƒæ•°æ®è¿‡æ»¤    é‡æ’åº      å†å²å¯¹è¯ç®¡ç†            â”‚
â”‚              æ··åˆæ£€ç´¢     Top-K       Tokenç®¡ç†               â”‚
â”‚                    â†“            â†“            â†“               â”‚
â”‚              [LLMç”Ÿæˆ] â† [ä¸Šä¸‹æ–‡æ³¨å…¥] â† [è´¨é‡è¿½è¸ª]            â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” åˆ†é˜¶æ®µè¯¦è§£

### é˜¶æ®µä¸€ï¼šç”¨æˆ·è¾“å…¥ä¸ä¸Šä¸‹æ–‡å˜é‡å‡†å¤‡

#### 1.1 å‰ç«¯ - PromptEditor ç»„ä»¶

**æ–‡ä»¶ä½ç½®ï¼š** `web/app/components/base/prompt-editor/index.tsx`

```typescript
// ç”¨æˆ·å¯ä»¥åœ¨å¯è§†åŒ–ç¼–è¾‘å™¨ä¸­æ’å…¥ä¸Šä¸‹æ–‡å—
<PromptEditor
  contextBlock={{
    show: true,                    // æ˜¾ç¤ºä¸Šä¸‹æ–‡å—
    selectable: true,              // å¯é€‰æ‹©çŸ¥è¯†åº“
    datasets: [{id, name}, ...]    // å…³è”çš„æ•°æ®é›†
  }}
  historyBlock={{
    show: true,                    // æ˜¾ç¤ºå†å²å¯¹è¯å—
    history: {user: 'Human', assistant: 'Assistant'}
  }}
  queryBlock={{
    show: true,                    // æ˜¾ç¤ºæŸ¥è¯¢å—
  }}
/>
```

**æ”¯æŒçš„ç‰¹æ®Šå˜é‡ï¼š**
- `{{#context#}}` - çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
- `{{#histories#}}` - å¯¹è¯å†å²
- `{{#query#}}` - ç”¨æˆ·å½“å‰é—®é¢˜
- `{{è‡ªå®šä¹‰å˜é‡}}` - ç”¨æˆ·è‡ªå®šä¹‰è¾“å…¥

#### 1.2 åç«¯ - åˆå§‹åŒ–è¿è¡Œæ—¶çŠ¶æ€

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/app/apps/advanced_chat/app_generator.py`

```python
class AdvancedChatAppGenerator:
    def generate(self, ...):
        # åˆ›å»ºåº”ç”¨ç”Ÿæˆå®ä½“
        application_generate_entity = AdvancedChatAppGenerateEntity(
            task_id=str(uuid.uuid4()),
            query=query,                    # ç”¨æˆ·é—®é¢˜
            inputs=inputs,                  # è¾“å…¥å˜é‡
            conversation_id=conversation.id, # å¯¹è¯ID
            files=files,                    # ä¸Šä¼ çš„æ–‡ä»¶
            ...
        )
        
        # åˆå§‹åŒ–å˜é‡æ±  - å­˜å‚¨æ‰€æœ‰ä¸Šä¸‹æ–‡å˜é‡
        # åç»­èŠ‚ç‚¹å¯ä»¥ä»å˜é‡æ± è·å–/è®¾ç½®å˜é‡
```

**å…³é”®æ•°æ®ç»“æ„ï¼š**
```python
SystemVariable(
    query="ç”¨æˆ·çš„é—®é¢˜",
    conversation_id="ä¼šè¯ID",
    user_id="ç”¨æˆ·ID",
    dialogue_count=1,  # å¯¹è¯è½®æ¬¡
    workflow_id="å·¥ä½œæµID",
    ...
)
```

---

### é˜¶æ®µäºŒï¼šçŸ¥è¯†åº“æ£€ç´¢ - æ„å»ºä¸Šä¸‹æ–‡

è¿™æ˜¯ä¸Šä¸‹æ–‡å·¥ç¨‹çš„**æ ¸å¿ƒé˜¶æ®µ**ï¼Œé€šè¿‡å¤šå±‚æ¬¡ä¼˜åŒ–ç¡®ä¿æ£€ç´¢è´¨é‡ã€‚

#### 2.1 çŸ¥è¯†æ£€ç´¢èŠ‚ç‚¹

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/workflow/nodes/knowledge_retrieval/knowledge_retrieval_node.py`

```python
class KnowledgeRetrievalNode(Node):
    def _run(self) -> NodeRunResult:
        # 1. ä»å˜é‡æ± æå–ç”¨æˆ·æŸ¥è¯¢
        variable = self.graph_runtime_state.variable_pool.get(
            self._node_data.query_variable_selector
        )
        query = variable.value
        
        # 2. æ‰§è¡ŒçŸ¥è¯†æ£€ç´¢
        results, usage = self._fetch_dataset_retriever(
            node_data=self._node_data, 
            query=query
        )
        
        # 3. è¾“å‡ºæ£€ç´¢ç»“æœåˆ°å˜é‡æ± 
        return NodeRunResult(
            outputs={"result": ArrayObjectSegment(value=results)}
        )
```

#### 2.2 å¤šå±‚æ¬¡æ£€ç´¢ä¼˜åŒ–

##### ğŸ¯ ä¼˜åŒ– 1ï¼šå…ƒæ•°æ®è¿‡æ»¤ï¼ˆMetadata Filteringï¼‰

**ä½œç”¨ï¼š** åœ¨å‘é‡æ£€ç´¢å‰å…ˆè¿‡æ»¤æ–‡æ¡£ï¼Œç¼©å°æ£€ç´¢èŒƒå›´

**ä¸‰ç§æ¨¡å¼ï¼š**

1. **Disabledï¼ˆç¦ç”¨ï¼‰** - ä¸è¿›è¡Œè¿‡æ»¤
2. **Automaticï¼ˆè‡ªåŠ¨ï¼‰** - LLM è‡ªåŠ¨ä»æŸ¥è¯¢ä¸­æå–è¿‡æ»¤æ¡ä»¶
3. **Manualï¼ˆæ‰‹åŠ¨ï¼‰** - ç”¨æˆ·é¢„è®¾è¿‡æ»¤è§„åˆ™

**è‡ªåŠ¨è¿‡æ»¤ç¤ºä¾‹ï¼š**

```python
# ç”¨æˆ·æŸ¥è¯¢ï¼š"2023å¹´çš„äº§å“æ–‡æ¡£ä¸­å…³äºAPIçš„å†…å®¹"
# 
# ç³»ç»Ÿè‡ªåŠ¨è°ƒç”¨ LLM æå–å…ƒæ•°æ®ï¼š
def _automatic_metadata_filter_func(self, dataset_ids, query, node_data):
    # ä½¿ç”¨ä¸“é—¨çš„ Prompt æ¨¡æ¿
    prompt_messages = self._get_prompt_template(
        metadata_fields=["year", "type", "category"],
        query=query
    )
    
    # LLM è¿”å›è¿‡æ»¤æ¡ä»¶
    result = model_instance.invoke_llm(prompt_messages)
    
    # è§£æ JSONï¼š
    # {
    #   "metadata_map": [
    #     {"metadata_field_name": "year", 
    #      "metadata_field_value": "2023",
    #      "comparison_operator": "="},
    #     {"metadata_field_name": "type", 
    #      "metadata_field_value": "product_doc",
    #      "comparison_operator": "="}
    #   ]
    # }
```

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/workflow/nodes/knowledge_retrieval/knowledge_retrieval_node.py:L523`

##### ğŸ¯ ä¼˜åŒ– 2ï¼šæ£€ç´¢ç­–ç•¥é€‰æ‹©

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/rag/retrieval/dataset_retrieval.py:L88`

**ä¸¤ç§æ¨¡å¼ï¼š**

```python
# æ¨¡å¼ä¸€ï¼šSingleï¼ˆå•æ•°æ®é›†è·¯ç”±ï¼‰
# é€‚ç”¨åœºæ™¯ï¼šå¤šä¸ªä¸“ä¸šé¢†åŸŸæ•°æ®é›†ï¼Œéœ€è¦æ™ºèƒ½é€‰æ‹©
if retrieve_config.retrieve_strategy == "SINGLE":
    # ä½¿ç”¨ LLM è¿›è¡Œè·¯ç”±å†³ç­–
    if model_supports_function_calling:
        # Function Calling è·¯ç”±
        planning_strategy = PlanningStrategy.ROUTER
    else:
        # ReAct è·¯ç”±
        planning_strategy = PlanningStrategy.REACT_ROUTER
    
    all_documents = self.single_retrieve(
        available_datasets=datasets,
        query=query,
        planning_strategy=planning_strategy,
        ...
    )

# æ¨¡å¼äºŒï¼šMultipleï¼ˆå¤šæ•°æ®é›†å¹¶è¡Œæ£€ç´¢ï¼‰
# é€‚ç”¨åœºæ™¯ï¼šéœ€è¦ç»¼åˆå¤šä¸ªæ•°æ®æºçš„ä¿¡æ¯
elif retrieve_config.retrieve_strategy == "MULTIPLE":
    all_documents = self.multiple_retrieve(
        available_datasets=datasets,
        query=query,
        top_k=4,
        score_threshold=0.7,
        rerank_mode="reranking_model",  # æˆ– "weighted_score"
        weights={"vector_weight": 0.7, "keyword_weight": 0.3},
        ...
    )
```

**æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰ï¼š**

```python
# åœ¨ Multiple æ¨¡å¼ä¸‹ï¼Œå¯ä»¥å¯ç”¨æ··åˆæ£€ç´¢
# åŒæ—¶ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…

# å‘é‡æ£€ç´¢å¾—åˆ†
vector_scores = self.calculate_vector_score(query, documents)

# å…³é”®è¯æ£€ç´¢å¾—åˆ†
keyword_scores = self.calculate_keyword_score(query, documents)

# åŠ æƒåˆå¹¶
final_scores = (
    vector_scores * vector_weight + 
    keyword_scores * keyword_weight
)
```

##### ğŸ¯ ä¼˜åŒ– 3ï¼šé‡æ’åºï¼ˆRerankingï¼‰

**ä½œç”¨ï¼š** ä½¿ç”¨ä¸“é—¨çš„é‡æ’åºæ¨¡å‹æå‡ç›¸å…³æ€§æ’åºè´¨é‡

```python
# æ”¯æŒçš„é‡æ’åºæ¨¡å‹ï¼š
# - Cohere Rerank
# - Jina Rerank
# - BGE Rerankerï¼ˆæœ¬åœ°ï¼‰

if reranking_enabled:
    # å°† Top-N å€™é€‰æ–‡æ¡£å‘é€ç»™é‡æ’åºæ¨¡å‹
    reranked_documents = rerank_model.rerank(
        query=query,
        documents=candidate_documents,
        top_n=top_k
    )
```

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/rag/rerank/rerank_model.py`

##### ğŸ¯ ä¼˜åŒ– 4ï¼šTop-K ä¸é˜ˆå€¼è¿‡æ»¤

```python
# åœ¨ DatasetRetrieval.retrieve() æ–¹æ³•ä¸­ï¼š

# 1. æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åº
document_context_list = sorted(
    document_context_list, 
    key=lambda x: x.score or 0.0, 
    reverse=True
)

# 2. åº”ç”¨ Top-K é™åˆ¶
top_k_documents = document_context_list[:top_k]

# 3. åº”ç”¨åˆ†æ•°é˜ˆå€¼è¿‡æ»¤
filtered_documents = [
    doc for doc in top_k_documents 
    if doc.score >= score_threshold
]

# 4. æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
context_string = "\n".join([
    doc.content for doc in filtered_documents
])
```

#### 2.3 ä¸Šä¸‹æ–‡æ ¼å¼åŒ–

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/rag/retrieval/dataset_retrieval.py:L252`

```python
# æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

for record in records:
    segment = record.segment
    
    # å¦‚æœæ˜¯ Q&A æ ¼å¼
    if segment.answer:
        context += f"question:{segment.get_sign_content()}\n"
        context += f"answer:{segment.answer}\n\n"
    else:
        # æ™®é€šæ–‡æ¡£ç‰‡æ®µ
        context += f"{segment.get_sign_content()}\n\n"

# æœ€ç»ˆä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼š
"""
question: ä»€ä¹ˆæ˜¯RAG?
answer: RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢ç³»ç»Ÿå’Œç”Ÿæˆæ¨¡å‹çš„æŠ€æœ¯...

question: Difyæ”¯æŒå“ªäº›å‘é‡æ•°æ®åº“?
answer: Difyæ”¯æŒå¤šç§å‘é‡æ•°æ®åº“ï¼ŒåŒ…æ‹¬Weaviateã€Qdrantã€Milvus...

Difyçš„RAGç³»ç»Ÿå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. å¤šç§æ£€ç´¢ç­–ç•¥æ”¯æŒ
2. æ™ºèƒ½é‡æ’åº
3. å…ƒæ•°æ®è¿‡æ»¤
...
"""
```

---

### é˜¶æ®µä¸‰ï¼šPrompt ç»„è£… - ä¸Šä¸‹æ–‡æ³¨å…¥

#### 3.1 Prompt æ¨¡æ¿è§£æ

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/prompt/utils/prompt_template_parser.py`

```python
class PromptTemplateParser:
    """
    è¯†åˆ«å¹¶æå–æ¨¡æ¿ä¸­çš„å˜é‡
    
    æ”¯æŒçš„å˜é‡æ ¼å¼ï¼š
    - {{variable_name}}  # æ™®é€šå˜é‡
    - {{#context#}}      # ç‰¹æ®Šï¼šçŸ¥è¯†åº“ä¸Šä¸‹æ–‡
    - {{#histories#}}    # ç‰¹æ®Šï¼šå¯¹è¯å†å²
    - {{#query#}}        # ç‰¹æ®Šï¼šç”¨æˆ·æŸ¥è¯¢
    """
    
    # æ­£åˆ™è¡¨è¾¾å¼
    REGEX = re.compile(
        r'\{\{([a-zA-Z_][a-zA-Z0-9_]{0,29}|'
        r'#histories#|#query#|#context#)\}\}'
    )
    
    def extract(self):
        """æå–æ¨¡æ¿ä¸­æ‰€æœ‰å˜é‡"""
        return re.findall(self.REGEX, self.template)
    
    def format(self, inputs: dict) -> str:
        """å°†å˜é‡å€¼æ›¿æ¢åˆ°æ¨¡æ¿ä¸­"""
        return re.sub(
            self.REGEX, 
            lambda m: inputs.get(m.group(1), m.group(0)), 
            self.template
        )
```

#### 3.2 é«˜çº§ Prompt è½¬æ¢å™¨

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/prompt/advanced_prompt_transform.py`

è¿™æ˜¯ä¸Šä¸‹æ–‡å·¥ç¨‹çš„**å…³é”®ç±»**ï¼Œè´Ÿè´£å°†æ‰€æœ‰ä¸Šä¸‹æ–‡å˜é‡æ³¨å…¥åˆ° Prompt ä¸­ã€‚

```python
class AdvancedPromptTransform(PromptTransform):
    def get_prompt(
        self,
        prompt_template: list[ChatModelMessage],
        inputs: dict,
        query: str,
        context: str | None,         # çŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        memory: TokenBufferMemory,   # å¯¹è¯å†å²
        model_config: ModelConfig,
        ...
    ) -> list[PromptMessage]:
        """
        ç»„è£…æœ€ç»ˆçš„ Prompt Messages
        """
        prompt_messages = []
        
        for prompt_item in prompt_template:
            raw_prompt = prompt_item.text
            
            # 1. è§£ææ¨¡æ¿
            parser = PromptTemplateParser(
                template=raw_prompt, 
                with_variable_tmpl=True
            )
            
            # 2. æ³¨å…¥ä¸Šä¸‹æ–‡
            prompt_inputs = self._set_context_variable(
                context, parser, inputs
            )
            
            # 3. æ³¨å…¥å¯¹è¯å†å²
            if memory:
                prompt_inputs = self._set_histories_variable(
                    memory, parser, prompt_inputs, model_config
                )
            
            # 4. æ³¨å…¥ç”¨æˆ·æŸ¥è¯¢
            prompt_inputs = self._set_query_variable(
                query, parser, prompt_inputs
            )
            
            # 5. æ ¼å¼åŒ–æœ€ç»ˆ Prompt
            final_prompt = parser.format(prompt_inputs)
            
            prompt_messages.append(
                UserPromptMessage(content=final_prompt)
            )
        
        return prompt_messages
```

##### ğŸ”§ ä¸Šä¸‹æ–‡å˜é‡æ³¨å…¥

```python
def _set_context_variable(
    self, 
    context: str | None, 
    parser: PromptTemplateParser, 
    prompt_inputs: dict
) -> dict:
    """æ³¨å…¥çŸ¥è¯†åº“ä¸Šä¸‹æ–‡"""
    prompt_inputs = dict(prompt_inputs)
    
    if '#context#' in parser.variable_keys:
        if context:
            # æ³¨å…¥æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“å†…å®¹
            prompt_inputs['#context#'] = context
        else:
            # å¦‚æœæ²¡æœ‰æ£€ç´¢ç»“æœï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
            prompt_inputs['#context#'] = ""
    
    return prompt_inputs
```

##### ğŸ”§ å¯¹è¯å†å²æ³¨å…¥ï¼ˆToken ç®¡ç†ï¼‰

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/prompt/advanced_prompt_transform.py:L270`

```python
def _set_histories_variable(
    self,
    memory: TokenBufferMemory,
    memory_config: MemoryConfig,
    parser: PromptTemplateParser,
    prompt_inputs: dict,
    model_config: ModelConfig,
) -> dict:
    """
    æ³¨å…¥å¯¹è¯å†å² - åŒ…å«æ™ºèƒ½ Token ç®¡ç†
    """
    prompt_inputs = dict(prompt_inputs)
    
    if '#histories#' in parser.variable_keys:
        if memory:
            # **å…³é”®ï¼šè®¡ç®—å‰©ä½™ Token å®¹é‡**
            # å…ˆæ„å»ºä¸€ä¸ªä¸´æ—¶æ¶ˆæ¯ï¼Œè®¡ç®—å·²ä½¿ç”¨çš„ Token
            tmp_prompt = parser.format({'#histories#': "", **prompt_inputs})
            tmp_message = UserPromptMessage(content=tmp_prompt)
            
            # è®¡ç®—å‰©ä½™ Token
            rest_tokens = self._calculate_rest_token(
                [tmp_message], 
                model_config
            )
            
            # ä»å†å²è®°å½•ä¸­æå–é€‚é‡çš„å¯¹è¯
            histories = memory.get_history_prompt_text(
                max_token_limit=rest_tokens,
                message_limit=memory_config.window.size
            )
            
            # æ ¼å¼åŒ–å†å²å¯¹è¯
            # Human: é—®é¢˜1
            # Assistant: å›ç­”1
            # Human: é—®é¢˜2
            # Assistant: å›ç­”2
            
            prompt_inputs['#histories#'] = histories
        else:
            prompt_inputs['#histories#'] = ""
    
    return prompt_inputs
```

**TokenBufferMemory å·¥ä½œåŸç†ï¼š**

```python
# æ–‡ä»¶ä½ç½®ï¼šapi/core/memory/token_buffer_memory.py

class TokenBufferMemory:
    def get_history_prompt_text(
        self, 
        max_token_limit: int,
        message_limit: int
    ) -> str:
        """
        è·å–å¯¹è¯å†å²ï¼Œå—ä¸¤ä¸ªé™åˆ¶ï¼š
        1. Token æ•°é‡é™åˆ¶
        2. æ¶ˆæ¯æ•°é‡é™åˆ¶
        """
        messages = self.get_messages(message_limit)
        
        history_text = ""
        current_tokens = 0
        
        # ä»æœ€è¿‘çš„æ¶ˆæ¯å¼€å§‹ï¼Œé€æ¡æ·»åŠ 
        for message in reversed(messages):
            message_text = self._format_message(message)
            message_tokens = self._count_tokens(message_text)
            
            if current_tokens + message_tokens > max_token_limit:
                break  # è¶…å‡º Token é™åˆ¶ï¼Œåœæ­¢æ·»åŠ 
            
            history_text = message_text + history_text
            current_tokens += message_tokens
        
        return history_text
```

#### 3.3 æœ€ç»ˆ Prompt ç¤ºä¾‹

**å‡è®¾ç”¨æˆ·æ¨¡æ¿ï¼š**
```
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚

èƒŒæ™¯çŸ¥è¯†ï¼š
{{#context#}}

å¯¹è¯å†å²ï¼š
{{#histories#}}

ç”¨æˆ·é—®é¢˜ï¼š
{{#query#}}

è¯·åŸºäºèƒŒæ™¯çŸ¥è¯†å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
```

**ç»è¿‡è½¬æ¢åçš„æœ€ç»ˆ Promptï¼š**
```
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚

èƒŒæ™¯çŸ¥è¯†ï¼š
question: ä»€ä¹ˆæ˜¯RAG?
answer: RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢ç³»ç»Ÿå’Œç”Ÿæˆæ¨¡å‹çš„æŠ€æœ¯ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼ºç”Ÿæˆè´¨é‡ã€‚

question: Difyæ”¯æŒå“ªäº›å‘é‡æ•°æ®åº“?
answer: Difyæ”¯æŒå¤šç§å‘é‡æ•°æ®åº“ï¼ŒåŒ…æ‹¬Weaviateã€Qdrantã€Milvusã€PGVectorç­‰ã€‚

Difyçš„RAGç³»ç»Ÿå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. å¤šç§æ£€ç´¢ç­–ç•¥æ”¯æŒï¼ˆå•æ•°æ®é›†è·¯ç”±ã€å¤šæ•°æ®é›†å¹¶è¡Œï¼‰
2. æ™ºèƒ½é‡æ’åºåŠŸèƒ½
3. å…ƒæ•°æ®è¿‡æ»¤èƒ½åŠ›
4. æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰
5. Tokenæ™ºèƒ½ç®¡ç†

å¯¹è¯å†å²ï¼š
Human: Difyæ”¯æŒå“ªäº›LLM?
Assistant: Difyæ”¯æŒOpenAIã€Claudeã€Azure OpenAIã€Anthropicç­‰å¤šç§LLMæä¾›å•†ã€‚

ç”¨æˆ·é—®é¢˜ï¼š
RAGç³»ç»Ÿä¸­çš„é‡æ’åºæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

è¯·åŸºäºèƒŒæ™¯çŸ¥è¯†å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
```

---

### é˜¶æ®µå››ï¼šLLM ç”Ÿæˆ

#### 4.1 ä¸Šä¸‹æ–‡è´¨é‡è¿½è¸ª

**æ–‡ä»¶ä½ç½®ï¼š** `api/core/app/apps/advanced_chat/app_generator.py`

```python
# åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿä¼šè¿½è¸ªä¸Šä¸‹æ–‡è´¨é‡æŒ‡æ ‡

trace_manager.add_trace(
    trace_type="knowledge_retrieval",
    metadata={
        "query": query,
        "dataset_ids": dataset_ids,
        "retrieval_count": len(documents),
        "top_k": top_k,
        "score_threshold": score_threshold,
        "reranking_enabled": True,
        "retrieval_time_ms": elapsed_time,
        "documents": [
            {
                "content": doc.content[:100],  # å‰100å­—ç¬¦
                "score": doc.score,
                "dataset_id": doc.dataset_id,
                "document_id": doc.document_id,
                "segment_id": doc.segment_id,
            }
            for doc in documents
        ]
    }
)
```

#### 4.2 æµå¼ç”Ÿæˆ

```python
# LLM ç”Ÿæˆæ”¯æŒæµå¼è¾“å‡º
for chunk in model_instance.invoke(
    prompt_messages=final_prompt_messages,
    stream=True
):
    yield chunk
```

---

### é˜¶æ®µäº”ï¼šç»“æœå­˜å‚¨ä¸åé¦ˆ

#### 5.1 æ¶ˆæ¯å­˜å‚¨

```python
# å°†ç”¨æˆ·é—®é¢˜å’ŒAIå›ç­”å­˜å…¥æ•°æ®åº“
message = Message(
    conversation_id=conversation.id,
    query=query,
    answer=answer,
    message_metadata={
        "retriever_resources": [
            {
                "dataset_id": doc.dataset_id,
                "document_id": doc.document_id,
                "segment_id": doc.segment_id,
                "score": doc.score,
                "content": doc.content,
            }
            for doc in retrieved_documents
        ]
    }
)

db.session.add(message)
db.session.commit()
```

#### 5.2 åé¦ˆæœºåˆ¶

ç”¨æˆ·å¯ä»¥å¯¹å›ç­”è¿›è¡Œè¯„åˆ†ï¼Œç³»ç»Ÿä¼šè®°å½•ï¼š

```python
message_feedback = MessageFeedback(
    message_id=message.id,
    rating="like",  # or "dislike"
    content="å›ç­”å¾ˆå‡†ç¡®"
)

# è¿™äº›åé¦ˆå¯ç”¨äºï¼š
# 1. è°ƒæ•´æ£€ç´¢ç­–ç•¥
# 2. ä¼˜åŒ–é‡æ’åºæ¨¡å‹
# 3. æ”¹è¿› Prompt æ¨¡æ¿
```

---

## ğŸ“Š å®Œæ•´æµç¨‹æ—¶åºå›¾

å·²åœ¨ `analysis/context_engineering_qa_flow.puml` æ–‡ä»¶ä¸­åˆ›å»ºï¼Œå±•ç¤ºäº†å®Œæ•´çš„ 5 é˜¶æ®µæµç¨‹ã€‚

---

## ğŸ¯ 6 å¤§ä¸Šä¸‹æ–‡å·¥ç¨‹ä¼˜åŒ–æŠ€æœ¯æ€»ç»“

### 1. å…ƒæ•°æ®è¿‡æ»¤ï¼ˆMetadata Filteringï¼‰
- **ä½œç”¨ä½ç½®ï¼š** çŸ¥è¯†æ£€ç´¢å‰
- **ä¼˜åŒ–ç›®æ ‡ï¼š** ç¼©å°æ£€ç´¢èŒƒå›´ï¼Œæé«˜ç²¾å‡†åº¦
- **å®ç°æ–¹å¼ï¼š** è‡ªåŠ¨/æ‰‹åŠ¨è®¾ç½®è¿‡æ»¤æ¡ä»¶
- **æ ¸å¿ƒä»£ç ï¼š** `KnowledgeRetrievalNode._get_metadata_filter_condition()`

### 2. æ£€ç´¢ç­–ç•¥ï¼ˆRetrieval Strategyï¼‰
- **Singleè·¯ç”±ï¼š** LLMæ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„å•ä¸ªæ•°æ®é›†
- **Multipleå¹¶è¡Œï¼š** ä»å¤šä¸ªæ•°æ®é›†å¹¶è¡Œæ£€ç´¢ï¼Œåˆå¹¶ç»“æœ
- **æ··åˆæ£€ç´¢ï¼š** å‘é‡ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…
- **æ ¸å¿ƒä»£ç ï¼š** `DatasetRetrieval.retrieve()`

### 3. é‡æ’åºï¼ˆRerankingï¼‰
- **ä½œç”¨ä½ç½®ï¼š** åˆæ­¥æ£€ç´¢å
- **ä¼˜åŒ–ç›®æ ‡ï¼š** ä½¿ç”¨ä¸“é—¨æ¨¡å‹é‡æ–°æ’åºï¼Œæå‡ç›¸å…³æ€§
- **æ”¯æŒæ¨¡å‹ï¼š** Cohere, Jina, BGE
- **æ ¸å¿ƒä»£ç ï¼š** `api/core/rag/rerank/rerank_model.py`

### 4. Token ç®¡ç†
- **åŠ¨æ€è®¡ç®—ï¼š** æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å’Œå·²ç”¨ Token åŠ¨æ€è°ƒæ•´
- **ä¼˜å…ˆçº§ç­–ç•¥ï¼š** ä¿ç•™æœ€è¿‘çš„å¯¹è¯å†å²
- **é˜²æ­¢è¶…é™ï¼š** è‡ªåŠ¨è£å‰ªï¼Œé¿å…è¶…å‡º Token é™åˆ¶
- **æ ¸å¿ƒä»£ç ï¼š** `AdvancedPromptTransform._set_histories_variable()`

### 5. Prompt æ¨¡æ¿ç»„è£…
- **å˜é‡æ³¨å…¥ï¼š** `{{#context#}}`, `{{#histories#}}`, `{{#query#}}`
- **æ ¼å¼çµæ´»ï¼š** ç”¨æˆ·å¯è‡ªå®šä¹‰æ¨¡æ¿ç»“æ„
- **ç±»å‹å®‰å…¨ï¼š** æ­£åˆ™è¡¨è¾¾å¼éªŒè¯å˜é‡æ ¼å¼
- **æ ¸å¿ƒä»£ç ï¼š** `PromptTemplateParser.format()`

### 6. è´¨é‡è¿½è¸ª
- **æ£€ç´¢è¿½è¸ªï¼š** è®°å½•æ£€ç´¢çš„æ•°æ®é›†ã€æ–‡æ¡£ã€åˆ†æ•°
- **æ€§èƒ½ç›‘æ§ï¼š** è®°å½•æ£€ç´¢è€—æ—¶ã€Tokenä½¿ç”¨
- **ç”¨æˆ·åé¦ˆï¼š** æ”¶é›†ç‚¹èµ/ç‚¹è¸©ï¼Œç”¨äºä¼˜åŒ–
- **æ ¸å¿ƒä»£ç ï¼š** `trace_manager.add_trace()`

---

## ğŸ’¡ å®æˆ˜å»ºè®®

### é…ç½®æœ€ä½³å®è·µ

1. **Top-K è®¾ç½®**
   - å°å‹çŸ¥è¯†åº“ï¼ˆ< 1000æ–‡æ¡£ï¼‰ï¼šTop-K = 3-5
   - ä¸­å‹çŸ¥è¯†åº“ï¼ˆ1000-10000ï¼‰ï¼šTop-K = 5-8
   - å¤§å‹çŸ¥è¯†åº“ï¼ˆ> 10000ï¼‰ï¼šTop-K = 8-12

2. **Score Threshold**
   - ç²¾ç¡®åŒ¹é…åœºæ™¯ï¼šthreshold = 0.8
   - æ¨¡ç³ŠåŒ¹é…åœºæ™¯ï¼šthreshold = 0.6
   - æ¢ç´¢æ€§åœºæ™¯ï¼šthreshold = 0.4

3. **é‡æ’åºå¯ç”¨æ¡ä»¶**
   - Top-K > 5 æ—¶å»ºè®®å¯ç”¨
   - å¤šæ•°æ®é›†å¹¶è¡Œæ£€ç´¢æ—¶å¼ºçƒˆæ¨è
   - å¯¹å“åº”é€Ÿåº¦è¦æ±‚é«˜æ—¶å¯ç¦ç”¨

4. **å¯¹è¯å†å²çª—å£**
   - é—²èŠåœºæ™¯ï¼šä¿ç•™ 10-20 è½®
   - ä¸“ä¸šé—®ç­”ï¼šä¿ç•™ 5-10 è½®
   - ç´§å¯†ä¸Šä¸‹æ–‡ä¾èµ–ï¼šä¿ç•™ 15-30 è½®

### æ€§èƒ½ä¼˜åŒ–æŠ€å·§

1. **ç¼“å­˜æ£€ç´¢ç»“æœ**
   ```python
   # å¯¹ç›¸åŒæŸ¥è¯¢çš„æ£€ç´¢ç»“æœç¼“å­˜ 5 åˆ†é’Ÿ
   cache_key = f"retrieval:{hash(query)}:{dataset_ids}"
   cached_result = redis.get(cache_key)
   if cached_result:
       return json.loads(cached_result)
   ```

2. **å¼‚æ­¥å¹¶è¡Œæ£€ç´¢**
   ```python
   # å¤šæ•°æ®é›†å¹¶è¡Œæ£€ç´¢æ—¶ä½¿ç”¨å¼‚æ­¥
   async def parallel_retrieve():
       tasks = [
           retrieve_from_dataset(dataset_id, query)
           for dataset_id in dataset_ids
       ]
       results = await asyncio.gather(*tasks)
       return merge_results(results)
   ```

3. **æ™ºèƒ½åˆ†å—**
   ```python
   # å°†å¤§æ–‡æ¡£åˆ†å—æ—¶ï¼Œè€ƒè™‘é‡å åŒºåŸŸ
   chunk_size = 500  # Token
   chunk_overlap = 50  # é‡å  Tokenï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
   ```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶ç´¢å¼•

### æ ¸å¿ƒæ–‡ä»¶æ¸…å•

| æ–‡ä»¶è·¯å¾„ | æ ¸å¿ƒåŠŸèƒ½ | å…³é”®ç±»/æ–¹æ³• |
|---------|---------|------------|
| `api/core/prompt/advanced_prompt_transform.py` | Promptç»„è£…ä¸ä¸Šä¸‹æ–‡æ³¨å…¥ | `AdvancedPromptTransform.get_prompt()` |
| `api/core/rag/retrieval/dataset_retrieval.py` | çŸ¥è¯†æ£€ç´¢ç¼–æ’ | `DatasetRetrieval.retrieve()` |
| `api/core/workflow/nodes/knowledge_retrieval/knowledge_retrieval_node.py` | çŸ¥è¯†æ£€ç´¢èŠ‚ç‚¹ | `KnowledgeRetrievalNode._run()` |
| `api/core/prompt/utils/prompt_template_parser.py` | æ¨¡æ¿å˜é‡è§£æ | `PromptTemplateParser.format()` |
| `api/core/memory/token_buffer_memory.py` | Tokenç®¡ç†å†…å­˜ | `TokenBufferMemory.get_history_prompt_text()` |
| `web/app/components/base/prompt-editor/` | å‰ç«¯Promptç¼–è¾‘å™¨ | `PromptEditor` ç»„ä»¶ |
| `api/core/app/apps/advanced_chat/app_generator.py` | å¯¹è¯åº”ç”¨ç”Ÿæˆå™¨ | `AdvancedChatAppGenerator.generate()` |

### æµ‹è¯•æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | æµ‹è¯•å†…å®¹ |
|---------|---------|
| `api/tests/unit_tests/core/prompt/test_advanced_prompt_transform.py` | Promptè½¬æ¢æµ‹è¯• |
| `api/tests/unit_tests/core/rag/test_dataset_retrieval.py` | æ£€ç´¢é€»è¾‘æµ‹è¯• |
| `api/tests/unit_tests/core/memory/test_token_buffer_memory.py` | Tokenç®¡ç†æµ‹è¯• |

---

## ğŸ“ æ€»ç»“

Dify é€šè¿‡**å¤šå±‚æ¬¡ã€ç³»ç»ŸåŒ–çš„ä¸Šä¸‹æ–‡å·¥ç¨‹æŠ€æœ¯**ï¼Œåœ¨çŸ¥è¯†é—®ç­”åœºæ™¯ä¸­å®ç°äº†ï¼š

âœ… **é«˜å‡†ç¡®æ€§** - é€šè¿‡å…ƒæ•°æ®è¿‡æ»¤å’Œé‡æ’åºç¡®ä¿æ£€ç´¢è´¨é‡  
âœ… **å¼ºè¿è´¯æ€§** - é€šè¿‡å¯¹è¯å†å²ç®¡ç†ä¿æŒä¸Šä¸‹æ–‡è¿ç»­  
âœ… **é«˜å¯æ§æ€§** - é€šè¿‡çµæ´»çš„Promptæ¨¡æ¿å¼•å¯¼LLMè¡Œä¸º  
âœ… **é«˜æ•ˆç‡** - é€šè¿‡æ™ºèƒ½Tokenç®¡ç†é¿å…æµªè´¹å’Œè¶…é™  
âœ… **å¯è¿½æº¯æ€§** - é€šè¿‡è´¨é‡è¿½è¸ªå®ç°é—®é¢˜è¯Šæ–­å’ŒæŒç»­ä¼˜åŒ–

è¿™äº›æŠ€æœ¯çš„æœ‰æœºç»“åˆï¼Œä½¿å¾— Dify èƒ½å¤Ÿåœ¨ä¼ä¸šçº§åº”ç”¨ä¸­æä¾›**å‡†ç¡®ã€å¯é ã€é«˜æ•ˆ**çš„çŸ¥è¯†é—®ç­”æœåŠ¡ã€‚

---

## ğŸ“š æ‰©å±•é˜…è¯»

- å‚è§ï¼š`analysis/context_engineering_qa_flow.puml` - å®Œæ•´æµç¨‹æ—¶åºå›¾
- å‚è§ï¼š`docs/zh-CN/` - Dify å®˜æ–¹ä¸­æ–‡æ–‡æ¡£
- å‚è§ï¼š`api/core/rag/README.md` - RAG ç³»ç»Ÿæ¶æ„è¯´æ˜