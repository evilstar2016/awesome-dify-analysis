# 知识库集成

## 1. 集成概述

Dify 支持多种外部知识源的接入，可以将外部数据导入到知识库中，实现 RAG 应用。

## 2. 支持的知识源

| 知识源 | 状态 | 说明 |
|--------|------|------|
| 本地文件上传 | ✅ 支持 | 支持多种文档格式 |
| Notion | ✅ 支持 | 导入 Notion 页面和数据库 |
| 网页爬取 | ✅ 支持 | 通过 URL 爬取网页内容 |
| Firecrawl | ✅ 支持 | 高质量网页爬取 |
| Jina Reader | ✅ 支持 | LLM 友好的网页提取 |
| API 集成 | ✅ 支持 | 通过 API 接入自定义数据源 |

## 3. 本地文件上传

### 3.1 支持的格式

- PDF (.pdf)
- Word (.docx)
- Excel (.xlsx)
- CSV (.csv)
- Markdown (.md)
- HTML (.html)
- Text (.txt)

### 3.2 上传流程

```
文件上传 → 格式检测 → 内容提取 → 文本分块 → 向量化 → 存入向量库
```

### 3.3 使用示例

```python
from core.rag.extractor.extract_processor import ExtractProcessor
from core.rag.datasource.vdb.vector_factory import VectorFactory

# 提取文档内容
documents = ExtractProcessor.extract(
    file_path="/path/to/document.pdf",
    file_extension="pdf"
)

# 分块
from core.rag.splitter import TextSplitter
splitter = TextSplitter()
chunks = splitter.split_documents(documents)

# 获取嵌入
embeddings = model_manager.text_embedding.invoke(
    texts=[chunk.page_content for chunk in chunks]
)

# 存入向量库
vector = VectorFactory.create(...)
vector.create(texts=chunks, embeddings=embeddings)
```

## 4. Notion 集成

### 4.1 配置

```bash
# Notion 集成需要在 Notion 中创建集成并获取 token
NOTION_INTEGRATION_TOKEN=secret_xxx
```

### 4.2 使用示例

```python
from core.rag.extractor.notion_extractor import NotionExtractor

# 导入 Notion 页面
extractor = NotionExtractor(
    notion_workspace_id="workspace_id",
    notion_obj_id="page_id",
    notion_page_type="page",  # 或 "database"
    tenant_id="tenant_id"
)

documents = extractor.extract()
```

## 5. 网页爬取集成

### 5.1 使用 Firecrawl

```python
from core.rag.extractor.firecrawl import FirecrawlWebExtractor

# 爬取网页
extractor = FirecrawlWebExtractor(
    url="https://example.com",
    job_id="job_123",
    tenant_id="tenant_id",
    mode="crawl",  # 或 "scrape"
    only_main_content=True
)

documents = extractor.extract()
```

### 5.2 使用 Jina Reader

```python
from core.rag.extractor.jina_reader_extractor import JinaReaderWebExtractor

extractor = JinaReaderWebExtractor(
    url="https://example.com",
    job_id="job_123",
    tenant_id="tenant_id",
    mode="crawl"
)

documents = extractor.extract()
```

## 6. API 集成

### 6.1 通过工具系统集成

```python
from core.tools.custom_tool import CustomTool

class DataSourceTool(CustomTool):
    """自定义数据源工具"""
    
    def _invoke(self, user_id: str, tool_parameters: dict):
        # 从 API 获取数据
        response = requests.get(
            "https://api.example.com/data",
            headers={"Authorization": f"Bearer {api_key}"}
        )
        
        data = response.json()
        
        # 转换为文档
        documents = self.convert_to_documents(data)
        
        # 存入知识库
        # ...
        
        return self.create_text_message(f"导入了 {len(documents)} 个文档")
```

## 7. 数据预处理

### 7.1 文本清洗

```python
from core.rag.cleaner import TextCleaner

cleaner = TextCleaner()
cleaned_text = cleaner.clean(raw_text)
```

### 7.2 文本分块

```python
from core.rag.splitter import FixedTextSplitter

splitter = FixedTextSplitter(
    chunk_size=800,
    chunk_overlap=100
)

chunks = splitter.split_documents(documents)
```

## 8. 增量更新

### 8.1 检测变更

```python
def sync_knowledge_base(source_id):
    """同步知识库"""
    # 获取上次同步时间
    last_sync = get_last_sync_time(source_id)
    
    # 获取变更的文档
    changed_docs = get_changed_documents(source_id, since=last_sync)
    
    # 删除旧文档
    for doc in changed_docs['deleted']:
        vector.delete_by_ids([doc.id])
    
    # 更新文档
    for doc in changed_docs['updated']:
        # 删除旧版本
        vector.delete_by_ids([doc.id])
        # 添加新版本
        add_document_to_vector(doc)
    
    # 添加新文档
    for doc in changed_docs['new']:
        add_document_to_vector(doc)
    
    # 更新同步时间
    update_last_sync_time(source_id)
```

## 9. 监控与日志

```python
import logging

logger = logging.getLogger(__name__)

def import_with_monitoring(source_type, source_id):
    """带监控的导入"""
    start_time = time.time()
    
    try:
        logger.info(f"Starting import from {source_type}: {source_id}")
        
        documents = extract_from_source(source_type, source_id)
        
        logger.info(f"Extracted {len(documents)} documents")
        
        # 处理和存储
        process_and_store(documents)
        
        latency = time.time() - start_time
        logger.info(f"Import completed in {latency:.2f}s")
        
    except Exception as e:
        logger.error(f"Import failed: {e}")
        raise
```

## 10. 最佳实践

### 10.1 数据质量

- 提取前验证文件格式
- 清洗和规范化文本
- 去除重复内容
- 保留必要的元数据

### 10.2 性能优化

- 批量处理文档
- 异步处理大型数据集
- 使用缓存避免重复提取
- 增量更新而非全量更新

### 10.3 错误处理

- 记录失败的文档
- 提供重试机制
- 通知用户处理进度

## 11. 参考资源

- [Notion API 文档](https://developers.notion.com/)
- [Firecrawl 文档](https://docs.firecrawl.dev/)
- [Jina Reader 文档](https://jina.ai/reader/)
