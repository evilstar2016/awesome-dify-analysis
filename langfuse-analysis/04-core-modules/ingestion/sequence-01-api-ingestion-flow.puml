@startuml API Ingestion Flow
title Ingestion 模块 - API 摄取流程
autonumber

actor "SDK Client" as SDK
participant "ingestion.ts\n(API Handler)" as API
participant "ApiAuthService" as Auth
participant "RateLimitService" as RateLimit
participant "processEventBatch" as Process
participant "createIngestionEventSchema\n(Zod)" as Schema
participant "sortBatch" as Sort
participant "S3 Storage" as S3
participant "IngestionQueue\n(BullMQ)" as Queue
participant "Redis" as Redis

== 1. 请求接收与预检查 ==

SDK -> API: POST /api/public/ingestion\n{ batch: [...], metadata: {...} }
activate API

API -> API: runMiddleware(cors)\n验证 CORS

API -> API: 检查 method === "POST"

API -> Auth: verifyAuthHeaderAndReturnScope()\n验证 Authorization header
activate Auth
Auth -> Auth: 解析 API key
Auth -> Auth: 查询数据库验证 key
Auth -> Auth: 检查 isIngestionSuspended\n(usage threshold)
Auth --> API: authCheck\n{ validKey, scope: { projectId, accessLevel } }
deactivate Auth

alt 认证失败
    API --> SDK: 401 Unauthorized
    note right: validKey=false 或\nmissing projectId
end

alt 摄取暂停
    API --> SDK: 403 Forbidden\n"Usage threshold exceeded"
    note right: isIngestionSuspended=true
end

== 2. 限流检查 ==

API -> RateLimit: rateLimitRequest(scope, "ingestion")
activate RateLimit
RateLimit -> Redis: 检查限流计数器\n(滑动窗口)
Redis --> RateLimit: 当前请求数
RateLimit -> RateLimit: 判断是否超限
RateLimit --> API: rateLimitCheck
deactivate RateLimit

alt 触发限流
    API --> SDK: 429 Too Many Requests
end

note over RateLimit: 限流采用 fail-open 模式\n如果 Redis 错误则放行

== 3. 请求体验证 ==

API -> API: batchType.safeParse(req.body)\n验证外层结构

alt 格式错误
    API --> SDK: 400 Bad Request\n{ message, errors: [...] }
end

== 4. 批量事件处理 ==

API -> Process: processEventBatch(batch, authCheck)
activate Process

note over Process: **阶段 1：验证和认证**

Process -> Schema: createIngestionEventSchema(isLangfuseInternal)
activate Schema
Schema --> Process: ingestionSchema\n(Zod schema)
deactivate Schema

loop 遍历每个事件
    Process -> Process: ingestionSchema.safeParse(event)
    
    alt 验证失败
        Process -> Process: 记录到 validationErrors
    else 验证成功
        Process -> Process: isAuthorized(event, authCheck)\n检查 accessLevel
        
        alt 权限不足
            Process -> Process: 记录到 authenticationErrors
        else 权限通过
            Process -> Process: 添加到 batch 数组
        end
    end
end

note over Process: 过滤 SDK_LOG 事件\n(仅记录日志，不处理)

note over Process: **阶段 2：排序和分组**

Process -> Sort: sortBatch(batch)
activate Sort
Sort -> Sort: 分离 create 和 update 事件
Sort -> Sort: 按 timestamp 升序排序
Sort -> Sort: create 事件优先，update 事件排后
Sort --> Process: sortedBatch
deactivate Sort

Process -> Process: 按 eventBodyId 分组\nsortedBatchByEventBodyId

note over Process: 分组规则：\nkey = entityType-eventBodyId\n同一 trace/observation/score\n的所有事件分到同一组

note over Process: **阶段 3：S3 异步上传**

loop 遍历每个 eventBodyId 组
    Process -> S3: uploadJson(bucketPath, events)\nPath: {prefix}{projectId}/{type}/{eventBodyId}/{key}.json
    activate S3
    
    alt 上传成功
        S3 --> Process: 成功
    else 上传失败
        S3 --> Process: 错误（如 SlowDown）
        note right: 检测 S3 SlowDown\n标记项目使用 secondary queue
    end
    deactivate S3
end

alt S3 上传失败
    Process --> API: throw Error\n"Failed to upload to blob storage"
    API --> SDK: 500 Internal Server Error
end

note over Process: **阶段 4：队列入队**

loop 遍历每个 eventBodyId 组
    Process -> Process: 计算 shardingKey\n= projectId-eventBodyId
    
    Process -> Queue: getInstance({ shardingKey })
    activate Queue
    Queue -> Queue: getShardIndex()\n计算分片索引
    Queue --> Process: queue instance\n(ingestion-queue-N)
    deactivate Queue
    
    Process -> Process: 判断 skipS3List\n= dataset_run_item OR\n(observation AND (otel OR in skipList))
    
    Process -> Process: isTraceIdInSample()\n检查采样配置
    
    alt 不在采样范围
        Process -> Process: 跳过入队\n记录 sampling metric
    else 在采样范围
        Process -> Queue: add(IngestionJob, payload, { delay })
        activate Queue
        
        note right of Queue: Job Payload:\n- type\n- eventBodyId\n- fileKey\n- skipS3List\n- forwardToEventsTable
        
        Queue -> Redis: 添加到 Redis queue\n(BullMQ)
        Redis --> Queue: job ID
        Queue --> Process: 成功
        deactivate Queue
        
        note over Process: delay 计算：\n- 23:45-00:15 UTC: 添加延迟\n- API 调用：min 5s\n- OTEL: 0s
    end
end

note over Process: **阶段 5：返回结果**

Process -> Process: aggregateBatchResult()\n汇总 successes/errors

Process --> API: { successes: [...], errors: [...] }
deactivate Process

== 5. 响应客户端 ==

API --> SDK: 207 Multi-Status\n{\n  successes: [{ id, status: 201 }],\n  errors: [{ id, status, message }]\n}
deactivate API

note over SDK,Redis: **后续流程：**\nWorker 从 Queue 消费 job\n→ 从 S3 下载事件\n→ 合并处理\n→ 写入 ClickHouse\n（详见 sequence-02）

@enduml
