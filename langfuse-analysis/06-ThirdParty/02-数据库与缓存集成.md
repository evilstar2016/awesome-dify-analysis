# 数据库与缓存集成

## 1. 集成概述

Langfuse 采用**多数据库架构**，结合 OLTP (在线事务处理) 和 OLAP (在线分析处理) 数据库，以及缓存和队列系统，实现高性能的 LLM 可观测性平台。

### 1.1 设计策略

- **读写分离**: PostgreSQL 处理事务，ClickHouse 处理分析查询
- **异步处理**: 使用 Redis + BullMQ 实现异步任务队列
- **缓存加速**: Redis 缓存热点数据和会话信息
- **数据同步**: Worker 负责 PostgreSQL → ClickHouse 数据同步

### 1.2 数据流

```
SDK/API Ingestion
    ↓
PostgreSQL (元数据) + S3 (原始事件)
    ↓
Redis Queue (BullMQ)
    ↓
Worker Processing
    ↓
ClickHouse (分析数据)
```

## 2. 支持的服务/产品

| 服务名称 | 类型 | SDK | 状态 | 优先级 | 用途 |
|---------|------|-----|------|--------|------|
| **PostgreSQL** | OLTP | `@prisma/client` | ✅ 已支持 | ⭐⭐⭐ 核心 | 事务数据、元数据 |
| **ClickHouse** | OLAP | `@clickhouse/client` | ✅ 已支持 | ⭐⭐⭐ 核心 | 观测数据、分析查询 |
| **Redis / Valkey** | Cache + Queue | `ioredis` | ✅ 已支持 | ⭐⭐⭐ 核心 | 缓存、任务队列 |
| **BullMQ** | Queue | `bullmq` | ✅ 已支持 | ⭐⭐⭐ 核心 | 异步任务处理 |

### 2.1 版本信息

```json
{
  "@prisma/client": "^6.17.1",
  "prisma": "^6.17.1",
  "@clickhouse/client": "^1.13.0",
  "ioredis": "^5.8.2",
  "bullmq": "^5.34.10",
  "kysely": "^0.27.4"
}
```

## 3. PostgreSQL 集成

### 3.1 技术栈

- **ORM**: Prisma 6.17.1
- **查询构建器**: Kysely (通过 prisma-extension-kysely)
- **连接池**: PgBouncer (推荐生产环境)

### 3.2 数据模型

```prisma
// packages/shared/prisma/schema.prisma

model Project {
  id          String   @id @default(cuid())
  name        String
  orgId       String?
  org         Organization? @relation(fields: [orgId], references: [id])
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt
  
  // 关联
  traces      Trace[]
  apiKeys     ApiKey[]
  prompts     Prompt[]
  datasets    Dataset[]
  // ...
}

model Trace {
  id          String   @id
  name        String?
  projectId   String
  project     Project  @relation(fields: [projectId], references: [id])
  userId      String?
  metadata    Json?
  tags        String[]
  bookmarked  Boolean  @default(false)
  public      Boolean  @default(false)
  timestamp   DateTime @default(now())
  
  observations Observation[]
  scores       Score[]
  // ...
}

// 50+ 其他模型
```

### 3.3 核心代码路径

```
packages/shared/
├── prisma/
│   ├── schema.prisma              # 数据库 Schema
│   ├── migrations/                # 迁移文件
│   └── generated/                 # Prisma Client 生成
├── src/
│   ├── db.ts                      # 数据库客户端初始化
│   └── server/repositories/       # 仓储模式封装
│       ├── traces.ts
│       ├── observations.ts
│       ├── scores.ts
│       └── ...
```

### 3.4 数据库客户端初始化

```typescript
// packages/shared/src/db.ts
import { Prisma, PrismaClient } from "@prisma/client";
import kyselyExtension from "prisma-extension-kysely";
import { PostgresDialect } from "kysely";
import { Pool } from "pg";

const createPrismaClient = () => {
  const client = new PrismaClient({
    log: env.NODE_ENV === "development" 
      ? ["query", "error", "warn"] 
      : ["error"],
  });

  // 添加 Kysely 扩展用于复杂查询
  return client.$extends(
    kyselyExtension({
      kysely: (driver) => ({
        dialect: new PostgresDialect({ driver }),
      }),
    })
  );
};

export const prisma = createPrismaClient();
export type ExtendedPrismaClient = ReturnType<typeof createPrismaClient>;
```

### 3.5 使用示例

```typescript
// 基础 CRUD
const trace = await prisma.trace.create({
  data: {
    id: "trace-123",
    projectId: "project-456",
    name: "API Call",
    userId: "user-789",
    metadata: { endpoint: "/api/chat" },
    tags: ["production", "api"]
  }
});

// 复杂查询 (使用 Kysely)
const results = await prisma.$kysely
  .selectFrom("traces")
  .leftJoin("observations", "observations.traceId", "traces.id")
  .select([
    "traces.id",
    "traces.name",
    sql<number>`COUNT(observations.id)`.as("observationCount")
  ])
  .where("traces.projectId", "=", projectId)
  .groupBy("traces.id")
  .execute();

// 事务
await prisma.$transaction([
  prisma.trace.create({ data: traceData }),
  prisma.observation.createMany({ data: observationsData })
]);
```

### 3.6 存储的数据

- **用户与组织**: users, organizations, memberships
- **项目与配置**: projects, apiKeys, prompts, datasets
- **元数据**: trace 基本信息，observation 关联关系
- **评估**: evalTemplates, jobConfigurations, llmApiKeys
- **计费**: subscriptions, usage records (企业版)

## 4. ClickHouse 集成

### 4.1 技术栈

- **客户端**: @clickhouse/client
- **迁移管理**: 自定义脚本 (packages/shared/clickhouse/migrations/)
- **查询构建**: Kysely (类型安全)

### 4.2 数据模型

```sql
-- packages/shared/clickhouse/migrations/001_observations.sql

CREATE TABLE IF NOT EXISTS observations (
  id String,
  trace_id String,
  project_id String,
  type Enum8('SPAN' = 1, 'GENERATION' = 2, 'EVENT' = 3),
  name String,
  start_time DateTime64(3),
  end_time Nullable(DateTime64(3)),
  completion_start_time Nullable(DateTime64(3)),
  model Nullable(String),
  model_parameters String DEFAULT '{}',
  input String DEFAULT '',
  output Nullable(String),
  metadata String DEFAULT '{}',
  level Enum8('DEBUG' = 1, 'DEFAULT' = 2, 'WARNING' = 3, 'ERROR' = 4) DEFAULT 'DEFAULT',
  status_message Nullable(String),
  version Nullable(String),
  
  -- 成本和使用
  input_cost Nullable(Decimal(20, 10)),
  output_cost Nullable(Decimal(20, 10)),
  total_cost Nullable(Decimal(20, 10)),
  usage_details String DEFAULT '{}',
  
  -- 时间戳
  created_at DateTime DEFAULT now(),
  updated_at DateTime DEFAULT now(),
  event_ts DateTime DEFAULT now(),
  is_deleted Int8 DEFAULT 0,
  
  -- 索引
  INDEX idx_project_id project_id TYPE bloom_filter GRANULARITY 1,
  INDEX idx_trace_id trace_id TYPE bloom_filter GRANULARITY 1,
  INDEX idx_name name TYPE tokenbf_v1(32768, 3, 0) GRANULARITY 1
) ENGINE = ReplacingMergeTree(updated_at, is_deleted)
ORDER BY (project_id, toStartOfHour(start_time), trace_id, id)
SETTINGS index_granularity = 8192;

-- 其他表: traces, scores, trace_sessions, ...
```

### 4.3 核心代码路径

```
packages/shared/
├── clickhouse/
│   ├── migrations/                # DDL 迁移文件
│   │   ├── 001_observations.sql
│   │   ├── 002_traces.sql
│   │   ├── 003_scores.sql
│   │   └── ...
│   └── scripts/
│       ├── up.sh                  # 执行迁移
│       ├── down.sh                # 回滚
│       └── drop.sh                # 删除所有表
├── src/
│   ├── tableDefinitions/          # Kysely 类型定义
│   │   ├── observations.ts
│   │   └── traces.ts
│   └── server/
│       ├── clickhouse/
│       │   ├── client.ts          # ClickHouse 客户端
│       │   ├── schema.ts          # Schema 工具
│       │   └── measureAndReturn.ts # 性能监控
│       └── repositories/
│           └── clickhouse.ts      # 仓储封装
```

### 4.4 客户端初始化

```typescript
// packages/shared/src/server/clickhouse/client.ts
import { createClient, type ClickHouseClient } from "@clickhouse/client";
import { env } from "../../env";

let clickhouseClient: ClickHouseClient | null = null;

export const getClickhouseClient = (): ClickHouseClient => {
  if (!clickhouseClient) {
    clickhouseClient = createClient({
      url: env.CLICKHOUSE_URL,
      username: env.CLICKHOUSE_USER,
      password: env.CLICKHOUSE_PASSWORD,
      database: env.CLICKHOUSE_DATABASE || "default",
      clickhouse_settings: {
        // 性能优化
        async_insert: env.CLICKHOUSE_ASYNC_INSERT ? 1 : 0,
        wait_for_async_insert: 0,
        async_insert_busy_timeout_ms: 1000,
      },
      compression: {
        request: env.CLICKHOUSE_COMPRESSION !== "false",
        response: env.CLICKHOUSE_COMPRESSION !== "false",
      },
    });
  }
  return clickhouseClient;
};
```

### 4.5 使用示例

```typescript
import { getClickhouseClient } from "@langfuse/shared/src/server/clickhouse/client";

// 插入数据
const client = getClickhouseClient();
await client.insert({
  table: "observations",
  values: [{
    id: "obs-123",
    trace_id: "trace-456",
    project_id: "project-789",
    type: "GENERATION",
    name: "OpenAI Chat",
    start_time: new Date().toISOString(),
    model: "gpt-4o",
    input: "Hello, world!",
    output: "Hi there!",
    total_cost: 0.0001,
    // ...
  }],
  format: "JSONEachRow"
});

// 查询数据
const result = await client.query({
  query: `
    SELECT 
      trace_id,
      COUNT(*) as observation_count,
      SUM(total_cost) as total_cost
    FROM observations
    WHERE project_id = {projectId:String}
      AND start_time >= {startTime:DateTime64(3)}
      AND start_time < {endTime:DateTime64(3)}
    GROUP BY trace_id
    ORDER BY total_cost DESC
    LIMIT 100
  `,
  query_params: {
    projectId: "project-789",
    startTime: startDate.toISOString(),
    endTime: endDate.toISOString()
  },
  format: "JSONEachRow"
});

const data = await result.json();
```

### 4.6 存储的数据

- **观测数据**: observations (SPAN, GENERATION, EVENT)
- **追踪数据**: traces (聚合数据，时间序列)
- **评分数据**: scores (评估结果)
- **会话数据**: trace_sessions (用户会话分析)
- **成本数据**: 成本计算和聚合

## 5. Redis 集成

### 5.1 技术栈

- **客户端**: ioredis 5.8.2
- **队列**: BullMQ 5.34.10
- **兼容性**: 支持 Redis 6.2+ / Valkey

### 5.2 核心代码路径

```
packages/shared/src/server/redis/
├── redis.ts                       # Redis 客户端
├── getQueue.ts                    # 队列工厂
├── traceUpsert.ts                 # Trace 更新队列
├── createEvalQueue.ts             # 评估队列
├── webhookQueue.ts                # Webhook 队列
├── traceDelete.ts                 # Trace 删除队列
├── cloudUsageMeteringQueue.ts     # 计费队列
└── ...
```

### 5.3 Redis 客户端

```typescript
// packages/shared/src/server/redis/redis.ts
import Redis, { type Redis as RedisType, type RedisOptions } from "ioredis";
import { env } from "../../env";

let redis: RedisType | null = null;

export const getRedis = (): RedisType => {
  if (!redis) {
    const config: RedisOptions = {
      host: env.REDIS_HOST || "localhost",
      port: Number(env.REDIS_PORT) || 6379,
      password: env.REDIS_AUTH || undefined,
      db: Number(env.REDIS_DB) || 0,
      maxRetriesPerRequest: null, // BullMQ 要求
      enableReadyCheck: false,
      retryStrategy(times) {
        const delay = Math.min(times * 50, 2000);
        return delay;
      },
    };

    // Redis Cluster 支持
    if (env.REDIS_CLUSTER_MODE === "true") {
      redis = new Redis.Cluster(
        [{ host: config.host, port: config.port }],
        {
          redisOptions: config,
          clusterRetryStrategy: config.retryStrategy,
        }
      );
    } else {
      redis = new Redis(config);
    }
  }
  return redis;
};
```

### 5.4 使用场景

#### 5.4.1 缓存

```typescript
import { getRedis } from "@langfuse/shared/src/server/redis/redis";

const redis = getRedis();

// 设置缓存
await redis.setex(
  `project:${projectId}:config`,
  3600, // TTL: 1 hour
  JSON.stringify(projectConfig)
);

// 读取缓存
const cached = await redis.get(`project:${projectId}:config`);
if (cached) {
  return JSON.parse(cached);
}

// 删除缓存
await redis.del(`project:${projectId}:config`);
```

#### 5.4.2 速率限制

```typescript
import { RateLimiterRedis } from "rate-limiter-flexible";

const rateLimiter = new RateLimiterRedis({
  storeClient: getRedis(),
  keyPrefix: "rate-limit",
  points: 100, // 100 requests
  duration: 60, // per 60 seconds
});

try {
  await rateLimiter.consume(userId);
  // 允许请求
} catch (error) {
  // 超过速率限制
  throw new Error("Too many requests");
}
```

#### 5.4.3 会话存储

```typescript
// NextAuth.js 使用 Redis 适配器 (可选)
import RedisAdapter from "@next-auth/redis-adapter";
import { getRedis } from "@langfuse/shared/src/server/redis/redis";

export const authOptions = {
  adapter: RedisAdapter(getRedis()),
  // ...
};
```

## 6. BullMQ 集成

### 6.1 队列架构

```
Web Application                Worker Service
      │                              │
      ├─ Enqueue ──→ Redis ──→ Process ─┤
      │              (Queue)            │
      │                                 ▼
      │                        ClickHouse / PostgreSQL
```

### 6.2 核心队列

| 队列名称 | 用途 | 处理器位置 |
|---------|------|-----------|
| `trace-upsert` | Trace 数据同步到 ClickHouse | `worker/src/queues/traceUpsert/` |
| `ingestion-queue` | 事件摄取处理 | `worker/src/queues/ingestionQueue/` |
| `evaluation-execution` | 执行评估任务 | `worker/src/queues/evalQueue/` |
| `webhook-queue` | Webhook 通知 | `worker/src/queues/webhookQueue/` |
| `batch-export` | 批量导出 | `worker/src/queues/batchExport/` |
| `dataset-run-item-upsert` | Dataset 项更新 | `worker/src/queues/datasetRunItemUpsert/` |

### 6.3 队列定义示例

```typescript
// packages/shared/src/server/redis/traceUpsert.ts
import { Queue } from "bullmq";
import { getRedis } from "./redis";
import { logger } from "../logger";

export const traceUpsertQueue = new Queue("trace-upsert", {
  connection: getRedis(),
  defaultJobOptions: {
    attempts: 5,
    backoff: {
      type: "exponential",
      delay: 1000, // 初始延迟 1 秒
    },
    removeOnComplete: {
      age: 3600, // 保留 1 小时
      count: 1000,
    },
    removeOnFail: {
      age: 86400, // 保留 1 天
    },
  },
});

// 入队任务
export const enqueueTraceUpsert = async (payload: {
  projectId: string;
  traceId: string;
}) => {
  await traceUpsertQueue.add("upsert-trace", payload, {
    jobId: `${payload.projectId}-${payload.traceId}`,
    removeOnComplete: true,
  });
  
  logger.debug("Enqueued trace upsert", payload);
};
```

### 6.4 Worker 处理器示例

```typescript
// worker/src/queues/traceUpsert/traceUpsertQueueProcessor.ts
import { Worker, Job } from "bullmq";
import { getRedis } from "@langfuse/shared/src/server/redis/redis";
import { getClickhouseClient } from "@langfuse/shared/src/server/clickhouse/client";
import { logger } from "@langfuse/shared/src/server/logger";

const worker = new Worker(
  "trace-upsert",
  async (job: Job) => {
    const { projectId, traceId } = job.data;

    logger.info("Processing trace upsert", { projectId, traceId });

    // 从 PostgreSQL 读取数据
    const trace = await prisma.trace.findUnique({
      where: { id: traceId },
      include: { observations: true, scores: true }
    });

    if (!trace) {
      throw new Error(`Trace ${traceId} not found`);
    }

    // 写入 ClickHouse
    const client = getClickhouseClient();
    await client.insert({
      table: "traces",
      values: [mapTraceToClickHouse(trace)],
      format: "JSONEachRow"
    });

    logger.info("Trace upsert completed", { projectId, traceId });
  },
  {
    connection: getRedis(),
    concurrency: 10, // 并发处理 10 个任务
    limiter: {
      max: 100,
      duration: 1000, // 每秒最多处理 100 个任务
    },
  }
);

worker.on("completed", (job) => {
  logger.debug("Job completed", { jobId: job.id });
});

worker.on("failed", (job, err) => {
  logger.error("Job failed", { 
    jobId: job?.id, 
    error: err.message,
    stack: err.stack 
  });
});
```

## 7. 配置管理

### 7.1 环境变量

```bash
# PostgreSQL
DATABASE_URL="postgresql://user:pass@localhost:5432/langfuse"
DATABASE_HOST="localhost"  # 可选，用于 PgBouncer
DATABASE_NAME="langfuse"

# ClickHouse
CLICKHOUSE_URL="http://localhost:8123"
CLICKHOUSE_USER="default"
CLICKHOUSE_PASSWORD="password"
CLICKHOUSE_DATABASE="default"
CLICKHOUSE_MIGRATION_URL="http://localhost:9000"  # 迁移用
CLICKHOUSE_ASYNC_INSERT="true"  # 异步插入
CLICKHOUSE_COMPRESSION="true"   # 启用压缩

# Redis
REDIS_HOST="localhost"
REDIS_PORT="6379"
REDIS_AUTH="password"
REDIS_DB="0"
REDIS_CLUSTER_MODE="false"

# Redis Connection String (替代方案)
REDIS_CONNECTION_STRING="redis://:password@localhost:6379/0"
```

### 7.2 连接池配置

```typescript
// Prisma 连接池 (通过 DATABASE_URL 参数)
DATABASE_URL="postgresql://user:pass@localhost:5432/langfuse?connection_limit=20&pool_timeout=10"

// ClickHouse 连接池 (SDK 内置)
const client = createClient({
  url: env.CLICKHOUSE_URL,
  max_open_connections: 10,  // 最大连接数
  // ...
});

// Redis 连接池 (ioredis 自动管理)
```

## 8. 性能优化

### 8.1 PostgreSQL 优化

```sql
-- 创建索引
CREATE INDEX CONCURRENTLY idx_traces_project_id_timestamp 
  ON traces (project_id, timestamp DESC);

CREATE INDEX CONCURRENTLY idx_observations_trace_id 
  ON observations (trace_id);

-- 使用 EXPLAIN ANALYZE 分析查询
EXPLAIN ANALYZE
SELECT * FROM traces 
WHERE project_id = 'proj-123' 
  AND timestamp > NOW() - INTERVAL '7 days';

-- 定期 VACUUM
VACUUM ANALYZE traces;
```

### 8.2 ClickHouse 优化

```sql
-- 使用 ReplacingMergeTree 去重
ENGINE = ReplacingMergeTree(updated_at, is_deleted)

-- 使用 Materialized Views 加速聚合查询
CREATE MATERIALIZED VIEW daily_costs_mv
ENGINE = SummingMergeTree()
ORDER BY (project_id, date)
AS SELECT
  project_id,
  toDate(start_time) as date,
  sum(total_cost) as daily_cost
FROM observations
GROUP BY project_id, date;

-- 使用 FINAL 获取最新版本 (去重)
SELECT * FROM observations FINAL
WHERE project_id = 'proj-123';

-- 优化分区
ALTER TABLE observations
MODIFY TTL start_time + INTERVAL 90 DAY;  -- 90 天后删除
```

### 8.3 Redis 优化

```typescript
// 使用 Pipeline 批量操作
const pipeline = redis.pipeline();
for (const key of keys) {
  pipeline.get(key);
}
const results = await pipeline.exec();

// 使用 Lua 脚本保证原子性
const luaScript = `
  local key = KEYS[1]
  local value = redis.call('GET', key)
  if not value then
    redis.call('SET', key, ARGV[1], 'EX', ARGV[2])
    return 1
  end
  return 0
`;
await redis.eval(luaScript, 1, cacheKey, value, ttl);
```

## 9. 监控与维护

### 9.1 健康检查

```typescript
// Web 应用健康检查
app.get("/api/health", async (req, res) => {
  const checks = {
    postgres: false,
    clickhouse: false,
    redis: false,
  };

  try {
    await prisma.$queryRaw`SELECT 1`;
    checks.postgres = true;
  } catch (e) {}

  try {
    const ch = getClickhouseClient();
    await ch.ping();
    checks.clickhouse = true;
  } catch (e) {}

  try {
    await getRedis().ping();
    checks.redis = true;
  } catch (e) {}

  const healthy = Object.values(checks).every(v => v);
  res.status(healthy ? 200 : 503).json(checks);
});
```

### 9.2 监控指标

- **PostgreSQL**: 连接数、慢查询、死锁、表膨胀
- **ClickHouse**: 查询时间、插入速率、磁盘使用、合并状态
- **Redis**: 内存使用、命中率、连接数、队列长度
- **BullMQ**: 队列积压、处理速率、失败率、延迟

## 10. 备份与恢复

### 10.1 PostgreSQL 备份

```bash
# 逻辑备份
pg_dump -h localhost -U langfuse -d langfuse > backup.sql

# 恢复
psql -h localhost -U langfuse -d langfuse < backup.sql

# 连续归档 (推荐生产环境)
# 配置 postgresql.conf
wal_level = replica
archive_mode = on
archive_command = 'cp %p /mnt/backup/wal/%f'
```

### 10.2 ClickHouse 备份

```bash
# 使用 clickhouse-backup
clickhouse-backup create my_backup
clickhouse-backup upload my_backup

# 恢复
clickhouse-backup download my_backup
clickhouse-backup restore my_backup
```

### 10.3 Redis 备份

```bash
# RDB 快照 (自动)
# 配置 redis.conf
save 900 1      # 900 秒内至少 1 次修改
save 300 10     # 300 秒内至少 10 次修改
save 60 10000   # 60 秒内至少 10000 次修改

# 手动保存
redis-cli BGSAVE

# AOF 持久化 (推荐队列场景)
appendonly yes
appendfsync everysec
```

## 11. 故障排查

### 11.1 常见问题

**PostgreSQL 连接池耗尽**
```
Error: Can't reach database server at `localhost:5432`
```
- 解决：增加连接限制或使用 PgBouncer

**ClickHouse 插入慢**
```
Code: 252. DB::Exception: Too many parts
```
- 解决：优化分区策略，增加合并频率

**Redis OOM**
```
OOM command not allowed when used memory > 'maxmemory'
```
- 解决：增加内存或设置 eviction 策略

**BullMQ 队列积压**
```
Queue length: 100000+
```
- 解决：增加 Worker 并发数，优化处理逻辑

## 12. 最佳实践

1. **连接管理**
   - 使用连接池
   - 正确关闭连接
   - 监控连接数

2. **查询优化**
   - 创建合适的索引
   - 避免 N+1 查询
   - 使用批量操作

3. **数据保留**
   - 设置 TTL 策略
   - 定期清理历史数据
   - 归档冷数据

4. **高可用**
   - PostgreSQL: 主从复制
   - ClickHouse: 分布式表和副本
   - Redis: 哨兵或集群模式

---

**更新日期**: 2025-12-17  
**维护者**: Langfuse Engineering Team
