# 大语言模型集成

## 1. 集成概述

Langfuse 深度集成了多个主流大语言模型提供商，通过 **Langchain** 框架提供统一的接口层，支持对话补全、流式响应、结构化输出和工具调用等功能。这些集成主要用于：

- **LLM Playground**: 交互式测试和迭代工具
- **评估系统**: LLM-as-a-judge 评估
- **内部功能**: 自动化任务和智能辅助

### 1.1 设计原则

- **统一接口**: 通过 Langchain 抽象层实现所有 LLM 的一致调用方式
- **安全存储**: 用户提供的 API Key 加密存储在数据库中
- **灵活配置**: 支持自定义 Base URL、额外 Headers 和提供商特定选项
- **可观测性**: 内置追踪和监控，可选择性地将内部 LLM 调用记录到 Langfuse

## 2. 支持的服务/产品

| 服务名称 | Adapter | SDK | 状态 | 优先级 | 支持的功能 |
|---------|---------|-----|------|--------|----------|
| **OpenAI** | `OpenAI` | `@langchain/openai` | ✅ 已支持 | ⭐⭐⭐ 核心 | Chat, Stream, Tools, Structured Output |
| **Azure OpenAI** | `Azure` | `@langchain/openai` | ✅ 已支持 | ⭐⭐⭐ 核心 | Chat, Stream, Tools, Structured Output |
| **Anthropic Claude** | `Anthropic` | `@langchain/anthropic` | ✅ 已支持 | ⭐⭐⭐ 核心 | Chat, Stream, Tools, Structured Output |
| **Google Vertex AI** | `VertexAI` | `@langchain/google-vertexai` | ✅ 已支持 | ⭐⭐ 重要 | Chat, Stream, Tools |
| **AWS Bedrock** | `Bedrock` | `@langchain/aws` | ✅ 已支持 | ⭐⭐ 重要 | Chat, Stream, Tools |
| **Google Gemini** | `GoogleAIStudio` | `@langchain/google-genai` | ✅ 已支持 | ⭐⭐ 重要 | Chat, Stream |

### 2.1 版本信息

```json
{
  "@langchain/openai": "^0.5.13",
  "@langchain/anthropic": "^0.3.32",
  "@langchain/google-vertexai": "^0.2.12",
  "@langchain/aws": "^0.1.15",
  "@langchain/google-genai": "^0.2.12",
  "@langchain/core": "^0.3.58",
  "langchain": "^0.3.30"
}
```

## 3. 集成方式

### 3.1 架构设计

```
┌─────────────────────────────────────────────────────────────┐
│                    Application Layer                         │
│  (Web UI / tRPC API / Worker Background Jobs)               │
└───────────────────────────┬─────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│            fetchLLMCompletion (Unified Interface)            │
│  packages/shared/src/server/llm/fetchLLMCompletion.ts       │
├─────────────────────────────────────────────────────────────┤
│  • Message transformation                                    │
│  • Configuration decryption                                  │
│  • Proxy support                                             │
│  • Internal tracing (optional)                               │
│  • Retry logic                                               │
└───────────────────────────┬─────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                    Langchain Layer                           │
├─────────────────────────────────────────────────────────────┤
│  ChatOpenAI         │  ChatAnthropic  │  ChatVertexAI      │
│  AzureChatOpenAI    │  ChatBedrock    │  ChatGoogleGen...  │
└───────────────────────────┬─────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                  External LLM APIs                           │
│  OpenAI API / Anthropic API / Google Cloud / AWS Bedrock    │
└─────────────────────────────────────────────────────────────┘
```

### 3.2 统一接口定义

```typescript
// 核心接口
async function fetchLLMCompletion(
  params: {
    messages: ChatMessage[];
    modelParams: ModelParams;
    llmConnection: {
      secretKey: string;          // 加密的 API Key
      extraHeaders?: string | null;
      baseURL?: string | null;
      config?: Record<string, string> | null;
    };
    streaming: boolean;
    tools?: LLMToolDefinition[];
    structuredOutputSchema?: ZodSchema | LLMJSONSchema;
    callbacks?: BaseCallbackHandler[];
    maxRetries?: number;
    traceSinkParams?: TraceSinkParams;
  }
): Promise<string | StreamResponse | StructuredOutput | ToolCallResponse>

// 消息格式
interface ChatMessage {
  role: 'system' | 'user' | 'assistant' | 'tool' | 'developer';
  content: string | MultimodalContent;
  toolCalls?: ToolCall[];
  toolCallId?: string;
}

// 模型参数
interface ModelParams {
  adapter: LLMAdapter;
  model: string;
  temperature?: number;
  max_tokens?: number;
  top_p?: number;
  providerOptions?: Record<string, any>;
}
```

## 4. 代码实现

### 4.1 核心代码路径

```
packages/shared/src/server/llm/
├── fetchLLMCompletion.ts      # 主入口，统一接口
├── types.ts                   # 类型定义
├── errors.ts                  # 错误处理
├── utils.ts                   # 工具函数
├── compileChatMessages.ts     # 消息格式转换
├── testModelCall.ts           # 测试 LLM 连接
└── getInternalTracingHandler.ts # 内部追踪处理

packages/shared/src/interfaces/
└── customLLMProviderConfigSchemas.ts  # 配置 Schema 定义

web/src/features/llm-api-key/
├── server/
│   └── llmApiKeyRouter.ts     # tRPC API 路由
└── components/
    └── LLMApiKeyForm.tsx      # 配置表单 UI
```

### 4.2 OpenAI 适配器实现

```typescript
// packages/shared/src/server/llm/fetchLLMCompletion.ts
if (modelParams.adapter === LLMAdapter.OpenAI) {
  const processedBaseURL = processOpenAIBaseURL({
    url: baseURL,
    modelName: modelParams.model,
  });

  chatModel = new ChatOpenAI({
    openAIApiKey: apiKey,
    modelName: modelParams.model,
    temperature: modelParams.temperature,
    maxTokens: modelParams.max_tokens,
    topP: modelParams.top_p,
    streamUsage: false,
    callbacks: finalCallbacks,
    maxRetries: maxRetries,
    timeout: timeoutMs,
    configuration: {
      baseURL: processedBaseURL,
      defaultHeaders: extraHeaders,
      ...(proxyAgent && { httpAgent: proxyAgent }),
    },
  });
}
```

### 4.3 Anthropic Claude 适配器实现

```typescript
if (modelParams.adapter === LLMAdapter.Anthropic) {
  const isClaude45Family =
    modelParams.model?.includes("claude-sonnet-4-5") ||
    modelParams.model?.includes("claude-opus-4-1") ||
    // ... 其他 4.x 版本

  chatModel = new ChatAnthropic({
    anthropicApiKey: apiKey,
    anthropicApiUrl: baseURL ?? undefined,
    modelName: modelParams.model,
    maxTokens: modelParams.max_tokens,
    callbacks: finalCallbacks,
    clientOptions: {
      maxRetries,
      timeout: timeoutMs,
      ...(proxyAgent && { httpAgent: proxyAgent }),
    },
    temperature: modelParams.temperature,
    topP: modelParams.top_p,
    invocationKwargs: modelParams.providerOptions,
  });

  // Claude 4.5 系列特殊处理：topP 和 temperature 不能同时设置
  if (isClaude45Family) {
    if (chatModel.topP === -1) chatModel.topP = undefined;
    if (modelParams.temperature !== undefined && modelParams.top_p === undefined) {
      chatModel.topP = undefined;
    }
    if (modelParams.top_p !== undefined && modelParams.temperature === undefined) {
      chatModel.temperature = undefined;
    }
  }
}
```

### 4.4 Google Vertex AI 适配器实现

```typescript
if (modelParams.adapter === LLMAdapter.VertexAI) {
  const parsedConfig = VertexAIConfigSchema.safeParse(config);
  
  if (parsedConfig.success && parsedConfig.data !== VERTEXAI_USE_DEFAULT_CREDENTIALS) {
    // 使用服务账号密钥
    const gcpServiceAccountKey = GCPServiceAccountKeySchema.parse(
      JSON.parse(parsedConfig.data),
    );

    chatModel = new ChatVertexAI({
      authOptions: {
        credentials: {
          client_email: gcpServiceAccountKey.client_email,
          private_key: gcpServiceAccountKey.private_key,
        },
        projectId: gcpServiceAccountKey.project_id,
      },
      model: modelParams.model,
      temperature: modelParams.temperature,
      maxOutputTokens: modelParams.max_tokens,
      callbacks: finalCallbacks,
      maxRetries,
      timeout: timeoutMs,
    });
  } else {
    // 使用默认凭证 (ADC - Application Default Credentials)
    chatModel = new ChatVertexAI({
      model: modelParams.model,
      temperature: modelParams.temperature,
      maxOutputTokens: modelParams.max_tokens,
      callbacks: finalCallbacks,
      maxRetries,
      timeout: timeoutMs,
    });
  }
}
```

### 4.5 AWS Bedrock 适配器实现

```typescript
if (modelParams.adapter === LLMAdapter.Bedrock) {
  const parsedCredential = BedrockCredentialSchema.safeParse(config?.credential);
  const parsedConfig = BedrockConfigSchema.safeParse(config);

  if (
    parsedCredential.success &&
    parsedCredential.data !== BEDROCK_USE_DEFAULT_CREDENTIALS
  ) {
    // 使用显式凭证
    chatModel = new ChatBedrockConverse({
      region: parsedConfig.success ? parsedConfig.data.region : undefined,
      credentials: {
        accessKeyId: parsedCredential.data.accessKeyId,
        secretAccessKey: apiKey,
        sessionToken: parsedCredential.data.sessionToken,
      },
      model: modelParams.model,
      temperature: modelParams.temperature,
      maxTokens: modelParams.max_tokens,
      callbacks: finalCallbacks,
      maxRetries,
      timeout: timeoutMs,
    });
  } else {
    // 使用 IAM 角色 / 默认凭证链
    chatModel = new ChatBedrockConverse({
      region: parsedConfig.success ? parsedConfig.data.region : undefined,
      model: modelParams.model,
      temperature: modelParams.temperature,
      maxTokens: modelParams.max_tokens,
      callbacks: finalCallbacks,
      maxRetries,
      timeout: timeoutMs,
    });
  }
}
```

## 5. 配置示例

### 5.1 用户配置 (Web UI)

用户在 Web UI 中配置 LLM 连接，存储在数据库的 `llm_api_keys` 表中：

```sql
CREATE TABLE llm_api_keys (
  id TEXT PRIMARY KEY,
  project_id TEXT NOT NULL,
  secret_key TEXT NOT NULL,  -- 加密存储
  provider LlmApiKeyProvider NOT NULL,
  adapter LlmAdapter NOT NULL,
  display_secret_key TEXT NOT NULL,  -- 脱敏显示，如 "sk-...abc123"
  base_url TEXT,
  custom_models TEXT[],
  with_default_models BOOLEAN DEFAULT true,
  config JSONB,
  extra_headers TEXT,  -- 加密存储
  created_at TIMESTAMP NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

### 5.2 OpenAI 配置

```typescript
// Web UI 提交
{
  provider: "openai",
  adapter: "OpenAI",
  secretKey: "sk-proj-...",  // 将被加密
  baseURL: null,  // 使用默认 https://api.openai.com/v1
  customModels: [],
  withDefaultModels: true
}

// 调用时
fetchLLMCompletion({
  messages: [
    { role: "user", content: "Hello!" }
  ],
  modelParams: {
    adapter: LLMAdapter.OpenAI,
    model: "gpt-4o",
    temperature: 0.7,
    max_tokens: 1000
  },
  llmConnection: {
    secretKey: encryptedKey,  // 从数据库读取
    baseURL: null,
    extraHeaders: null,
    config: null
  },
  streaming: false
})
```

### 5.3 Azure OpenAI 配置

```typescript
{
  provider: "azure-openai",
  adapter: "Azure",
  secretKey: "your-azure-key",
  baseURL: "https://your-resource.openai.azure.com/openai/deployments/your-deployment",
  customModels: ["gpt-4", "gpt-35-turbo"],
  withDefaultModels: false
}
```

### 5.4 Anthropic Claude 配置

```typescript
{
  provider: "anthropic",
  adapter: "Anthropic",
  secretKey: "sk-ant-api03-...",
  baseURL: null,  // 使用默认 https://api.anthropic.com
  customModels: [],
  withDefaultModels: true
}
```

### 5.5 Google Vertex AI 配置 (服务账号)

```typescript
{
  provider: "google",
  adapter: "VertexAI",
  secretKey: "",  // 不需要
  baseURL: null,
  config: JSON.stringify({
    type: "service_account",
    project_id: "your-project-id",
    private_key_id: "...",
    private_key: "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n",
    client_email: "sa@your-project.iam.gserviceaccount.com",
    client_id: "...",
    auth_uri: "https://accounts.google.com/o/oauth2/auth",
    token_uri: "https://oauth2.googleapis.com/token"
  })
}
```

### 5.6 AWS Bedrock 配置

```typescript
// 使用 IAM 角色 (推荐)
{
  provider: "aws-bedrock",
  adapter: "Bedrock",
  secretKey: "",  // 使用 IAM 角色
  config: {
    credential: "USE_DEFAULT_CREDENTIALS",
    region: "us-east-1"
  }
}

// 使用显式凭证
{
  provider: "aws-bedrock",
  adapter: "Bedrock",
  secretKey: "your-secret-access-key",  // 加密存储
  config: {
    credential: {
      accessKeyId: "AKIA...",
      sessionToken: "..." // 可选
    },
    region: "us-east-1"
  }
}
```

## 6. 使用示例

### 6.1 基础对话

```typescript
import { fetchLLMCompletion } from "@langfuse/shared/src/server/llm/fetchLLMCompletion";

const response = await fetchLLMCompletion({
  messages: [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "What is the capital of France?" }
  ],
  modelParams: {
    adapter: LLMAdapter.OpenAI,
    model: "gpt-4o",
    temperature: 0.7,
    max_tokens: 500
  },
  llmConnection: {
    secretKey: encryptedApiKey,
    baseURL: null,
    extraHeaders: null,
    config: null
  },
  streaming: false
});

console.log(response); // "The capital of France is Paris."
```

### 6.2 流式响应

```typescript
const stream = await fetchLLMCompletion({
  messages: [
    { role: "user", content: "Tell me a story." }
  ],
  modelParams: {
    adapter: LLMAdapter.Anthropic,
    model: "claude-3-5-sonnet-20241022",
    temperature: 0.9,
    max_tokens: 2000
  },
  llmConnection: {
    secretKey: encryptedApiKey,
    baseURL: null,
    extraHeaders: null,
    config: null
  },
  streaming: true
});

// 处理流式数据
for await (const chunk of stream) {
  const text = new TextDecoder().decode(chunk);
  process.stdout.write(text);
}
```

### 6.3 结构化输出

```typescript
import { z } from "zod";

const schema = z.object({
  name: z.string(),
  age: z.number(),
  city: z.string()
});

const result = await fetchLLMCompletion({
  messages: [
    { 
      role: "user", 
      content: "Extract person info: John is 30 years old and lives in NYC." 
    }
  ],
  modelParams: {
    adapter: LLMAdapter.OpenAI,
    model: "gpt-4o-2024-08-06",
    temperature: 0
  },
  llmConnection: {
    secretKey: encryptedApiKey,
    baseURL: null,
    extraHeaders: null,
    config: null
  },
  streaming: false,
  structuredOutputSchema: schema
});

console.log(result);
// { name: "John", age: 30, city: "New York City" }
```

### 6.4 工具调用 (Function Calling)

```typescript
const tools: LLMToolDefinition[] = [
  {
    name: "get_weather",
    description: "Get the current weather in a location",
    parameters: {
      type: "object",
      properties: {
        location: {
          type: "string",
          description: "The city name"
        }
      },
      required: ["location"]
    }
  }
];

const response = await fetchLLMCompletion({
  messages: [
    { role: "user", content: "What's the weather like in Paris?" }
  ],
  modelParams: {
    adapter: LLMAdapter.OpenAI,
    model: "gpt-4o",
    temperature: 0
  },
  llmConnection: {
    secretKey: encryptedApiKey,
    baseURL: null,
    extraHeaders: null,
    config: null
  },
  streaming: false,
  tools: tools
});

console.log(response);
// {
//   toolCalls: [{
//     name: "get_weather",
//     arguments: { location: "Paris" }
//   }]
// }
```

## 7. 错误处理

### 7.1 常见错误

| 错误类型 | HTTP Code | 原因 | 解决方案 |
|---------|-----------|------|----------|
| `AuthenticationError` | 401 | API Key 无效或过期 | 检查并更新 API Key |
| `RateLimitError` | 429 | 超过速率限制 | 降低请求频率或升级套餐 |
| `InvalidRequestError` | 400 | 请求参数错误 | 检查参数格式和模型支持 |
| `APIError` | 500+ | 服务端错误 | 重试或切换提供商 |
| `TimeoutError` | - | 请求超时 | 增加超时时间或检查网络 |
| `NetworkError` | - | 网络连接问题 | 检查网络连接和代理设置 |

### 7.2 错误处理实现

```typescript
// packages/shared/src/server/llm/errors.ts
export class LLMCompletionError extends Error {
  public readonly statusCode?: number;
  public readonly provider: string;
  public readonly originalError: any;

  constructor(
    message: string,
    provider: string,
    originalError?: any,
    statusCode?: number
  ) {
    super(message);
    this.name = "LLMCompletionError";
    this.provider = provider;
    this.originalError = originalError;
    this.statusCode = statusCode;
  }
}

// 使用
try {
  const response = await fetchLLMCompletion(params);
} catch (error) {
  if (error instanceof LLMCompletionError) {
    logger.error("LLM completion failed", {
      provider: error.provider,
      statusCode: error.statusCode,
      message: error.message
    });
    
    if (error.statusCode === 429) {
      // 处理速率限制
      await delay(60000); // 等待 1 分钟
      return retry();
    }
  }
  throw error;
}
```

### 7.3 重试策略

```typescript
// 配置重试
const response = await fetchLLMCompletion({
  ...params,
  maxRetries: 3  // 默认 2 次重试
});

// Langchain 内置指数退避重试
// 第 1 次失败：等待 4 秒
// 第 2 次失败：等待 8 秒
// 第 3 次失败：抛出错误
```

## 8. 性能优化

### 8.1 超时配置

```typescript
// packages/shared/src/env.ts
LANGFUSE_FETCH_LLM_COMPLETION_TIMEOUT_MS: z.coerce
  .number()
  .default(180_000), // 默认 3 分钟

// 所有 LLM 调用都受此超时限制
```

### 8.2 代理支持

```typescript
// 环境变量
HTTPS_PROXY=http://proxy.example.com:8080

// 自动应用到所有 LLM 客户端
const proxyAgent = proxyUrl ? new HttpsProxyAgent(proxyUrl) : undefined;
```

### 8.3 并发控制

```typescript
// 在应用层实现并发限制
import pLimit from 'p-limit';

const limit = pLimit(5); // 最多 5 个并发请求

const promises = requests.map(req => 
  limit(() => fetchLLMCompletion(req))
);

const results = await Promise.all(promises);
```

## 9. 监控与日志

### 9.1 内部追踪

Langfuse 可以将 LLM 调用追踪到自身：

```typescript
const response = await fetchLLMCompletion({
  ...params,
  traceSinkParams: {
    traceId: "eval-run-123",
    observationId: "eval-obs-456",
    projectId: "project-789",
    environment: "langfuse_evaluation",  // 必须以 "langfuse" 开头
    // ... 其他追踪参数
  }
});
```

### 9.2 日志记录

```typescript
// 使用 Winston 记录结构化日志
logger.info("LLM request", {
  adapter: modelParams.adapter,
  model: modelParams.model,
  messageCount: messages.length,
  streaming: streaming
});

logger.warn("LLM request retry", {
  adapter: modelParams.adapter,
  attempt: 2,
  error: error.message
});
```

## 10. 扩展新 LLM 提供商

### 10.1 扩展步骤

1. **添加 Langchain 依赖**
   ```bash
   cd packages/shared
   pnpm add @langchain/new-provider
   ```

2. **添加 Adapter 枚举**
   ```typescript
   // packages/shared/src/server/llm/types.ts
   export enum LLMAdapter {
     OpenAI = "OpenAI",
     Anthropic = "Anthropic",
     // ... 现有的
     NewProvider = "NewProvider",  // 新增
   }
   ```

3. **实现适配器逻辑**
   ```typescript
   // packages/shared/src/server/llm/fetchLLMCompletion.ts
   import { ChatNewProvider } from "@langchain/new-provider";

   // 在 fetchLLMCompletion 中添加
   if (modelParams.adapter === LLMAdapter.NewProvider) {
     chatModel = new ChatNewProvider({
       apiKey: apiKey,
       modelName: modelParams.model,
       temperature: modelParams.temperature,
       maxTokens: modelParams.max_tokens,
       callbacks: finalCallbacks,
       maxRetries: maxRetries,
       timeout: timeoutMs,
     });
   }
   ```

4. **添加配置 Schema (如需要)**
   ```typescript
   // packages/shared/src/interfaces/customLLMProviderConfigSchemas.ts
   export const NewProviderConfigSchema = z.object({
     // 提供商特定配置
   });
   ```

5. **更新 Web UI**
   
   在 `web/src/features/llm-api-key/components/` 中添加表单组件

6. **编写测试**
   ```typescript
   // worker/src/__tests__/llmConnections.test.ts
   describe("NewProvider LLM Connection", () => {
     it("should successfully call new provider", async () => {
       // 测试代码
     });
   });
   ```

## 11. 最佳实践

### 11.1 安全

1. **API Key 管理**
   - 始终加密存储 API Keys
   - 不在日志中记录 API Keys
   - 定期轮换密钥

2. **访问控制**
   - 通过项目权限控制 LLM 连接访问
   - 使用服务账号或 IAM 角色（云环境）

### 11.2 成本优化

1. **模型选择**
   - 根据任务复杂度选择合适的模型
   - 使用较小模型处理简单任务

2. **缓存**
   - 缓存相同输入的 LLM 响应（应用层实现）

3. **Token 管理**
   - 设置合理的 `max_tokens` 限制
   - 使用 `top_p` 或 `temperature` 控制输出长度

### 11.3 可靠性

1. **重试机制**
   - 使用内置重试功能
   - 处理速率限制错误

2. **降级策略**
   - 准备备用 LLM 提供商
   - 设置合理的超时时间

3. **错误处理**
   - 捕获并处理所有 LLM 错误
   - 提供友好的错误提示

## 12. 相关资源

- **Langchain 文档**: https://js.langchain.com/
- **OpenAI API 文档**: https://platform.openai.com/docs
- **Anthropic API 文档**: https://docs.anthropic.com/
- **Google Vertex AI 文档**: https://cloud.google.com/vertex-ai/docs
- **AWS Bedrock 文档**: https://docs.aws.amazon.com/bedrock/

---

**更新日期**: 2025-12-17  
**维护者**: Langfuse Engineering Team
