# çŸ¥è¯†åº“ç®¡ç†è¯¦ç»†è¯´æ˜ (Knowledge Base Management)

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£è¯¦ç»†æè¿° RAGFlow çŸ¥è¯†åº“ç®¡ç†æ¨¡å—çš„ä¸šåŠ¡æµç¨‹ã€æŠ€æœ¯å®ç°ã€API æ¥å£å’Œé…ç½®è¯´æ˜ã€‚

---

## ç›®å½•

- [1. åŠŸèƒ½æ¦‚è¿°](#1-åŠŸèƒ½æ¦‚è¿°)
- [2. æ ¸å¿ƒä¸šåŠ¡æµç¨‹](#2-æ ¸å¿ƒä¸šåŠ¡æµç¨‹)
- [3. æ•°æ®æ¨¡å‹è¯¦è§£](#3-æ•°æ®æ¨¡å‹è¯¦è§£)
- [4. API æ¥å£å®ç°](#4-api-æ¥å£å®ç°)
- [5. æœåŠ¡å±‚æ¶æ„](#5-æœåŠ¡å±‚æ¶æ„)
- [6. è§£æé…ç½®è¯¦è§£](#6-è§£æé…ç½®è¯¦è§£)
- [7. æƒé™æ§åˆ¶æœºåˆ¶](#7-æƒé™æ§åˆ¶æœºåˆ¶)
- [8. é”™è¯¯å¤„ç†](#8-é”™è¯¯å¤„ç†)
- [9. æ€§èƒ½ä¼˜åŒ–](#9-æ€§èƒ½ä¼˜åŒ–)
- [10. æœ€ä½³å®è·µ](#10-æœ€ä½³å®è·µ)

---

## 1. åŠŸèƒ½æ¦‚è¿°

### 1.1 æ¨¡å—å®šä½

çŸ¥è¯†åº“ç®¡ç†æ¨¡å—æ˜¯ RAGFlow çš„æ ¸å¿ƒåŸºç¡€æ¨¡å—ï¼Œè´Ÿè´£ï¼š
- ğŸ“¦ **çŸ¥è¯†åº“ç”Ÿå‘½å‘¨æœŸç®¡ç†**: åˆ›å»ºã€é…ç½®ã€æ›´æ–°ã€åˆ é™¤
- ğŸ“„ **æ–‡æ¡£ç®¡ç†**: ä¸Šä¼ ã€è§£æã€ç´¢å¼•ã€æ£€ç´¢
- ğŸ”§ **è§£æé…ç½®**: çµæ´»çš„æ–‡æ¡£è§£æç­–ç•¥é…ç½®
- ğŸ” **æƒé™æ§åˆ¶**: åŸºäºç§Ÿæˆ·å’Œå›¢é˜Ÿçš„æƒé™ç®¡ç†
- ğŸ“Š **ç»Ÿè®¡åˆ†æ**: æ–‡æ¡£æ•°é‡ã€åˆ†å—æ•°é‡ã€Token ç»Ÿè®¡

### 1.2 ä¸»è¦åŠŸèƒ½æ¨¡å—

```mermaid
graph TB
    KB[çŸ¥è¯†åº“ç®¡ç†]
    KB --> CREATE[åˆ›å»ºçŸ¥è¯†åº“]
    KB --> CONFIG[é…ç½®ç®¡ç†]
    KB --> DOC[æ–‡æ¡£ç®¡ç†]
    KB --> SEARCH[æ£€ç´¢æœåŠ¡]
    KB --> PERM[æƒé™ç®¡ç†]
    
    CREATE --> VALIDATE[åç§°éªŒè¯]
    CREATE --> PARSER[è§£æå™¨é…ç½®]
    CREATE --> EMBED[åµŒå…¥æ¨¡å‹é€‰æ‹©]
    
    DOC --> UPLOAD[æ–‡æ¡£ä¸Šä¼ ]
    DOC --> PARSE[æ–‡æ¡£è§£æ]
    DOC --> INDEX[å‘é‡ç´¢å¼•]
    
    SEARCH --> VECTOR[å‘é‡æ£€ç´¢]
    SEARCH --> FULL[å…¨æ–‡æ£€ç´¢]
    SEARCH --> GRAPH[å›¾è°±æ£€ç´¢]
```

### 1.3 æŠ€æœ¯ç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **å¼‚æ­¥å¤„ç†** | åŸºäº Quart æ¡†æ¶çš„å¼‚æ­¥ API |
| **åˆ†å¸ƒå¼ä»»åŠ¡** | Redis + Celery ä»»åŠ¡é˜Ÿåˆ— |
| **å¤šç§Ÿæˆ·æ”¯æŒ** | åŸºäº tenant_id çš„æ•°æ®éš”ç¦» |
| **çµæ´»é…ç½®** | JSON æ ¼å¼çš„è§£æå™¨é…ç½® |
| **å‘é‡æ£€ç´¢** | ES/OpenSearch/Infinity æ”¯æŒ |
| **å¯¹è±¡å­˜å‚¨** | MinIO S3 å…¼å®¹å­˜å‚¨ |

---

## 2. æ ¸å¿ƒä¸šåŠ¡æµç¨‹

### 2.1 çŸ¥è¯†åº“åˆ›å»ºæµç¨‹

#### æµç¨‹å›¾

![çŸ¥è¯†åº“åˆ›å»ºæµç¨‹](./01-create-kb-sequence.puml)

#### è¯¦ç»†æ­¥éª¤

**æ­¥éª¤ 1: è¯·æ±‚éªŒè¯**
```python
# æ–‡ä»¶: api/apps/kb_app.py
@manager.route('/create', methods=['post'])
@login_required
@validate_request("name")
async def create():
    req = await get_request_json()
    # æå–å¿…éœ€å‚æ•°: name
    # å¯é€‰å‚æ•°: description, language, parser_id, embd_id, parser_config
```

**æ­¥éª¤ 2: åç§°éªŒè¯ä¸å»é‡**
```python
# æ–‡ä»¶: api/db/services/knowledgebase_service.py
def create_with_name(cls, *, name: str, tenant_id: str, ...):
    # 1. éªŒè¯åç§°ç±»å‹
    if not isinstance(name, str):
        return False, get_data_error_result(message="Dataset name must be string.")
    
    # 2. éªŒè¯åç§°éç©º
    dataset_name = name.strip()
    if dataset_name == "":
        return False, get_data_error_result(message="Dataset name can't be empty.")
    
    # 3. éªŒè¯åç§°é•¿åº¦ (1024 å­—èŠ‚é™åˆ¶)
    if len(dataset_name.encode("utf-8")) > DATASET_NAME_LIMIT:
        return False, get_data_error_result(...)
    
    # 4. è‡ªåŠ¨å»é‡ (åœ¨åç§°åæ·»åŠ æ•°å­—)
    dataset_name = duplicate_name(
        cls.query,
        name=dataset_name,
        tenant_id=tenant_id,
        status=StatusEnum.VALID.value,
    )
    # ä¾‹å¦‚: "æˆ‘çš„æ–‡æ¡£" -> "æˆ‘çš„æ–‡æ¡£(1)" -> "æˆ‘çš„æ–‡æ¡£(2)"
```

**æ­¥éª¤ 3: è§£æå™¨é…ç½®æ„å»º**
```python
# ç”Ÿæˆ parser_config
payload["parser_config"] = get_parser_config(parser_id, kwargs.get("parser_config"))

# get_parser_config è¿”å›çš„é»˜è®¤é…ç½®:
{
    "chunk_token_num": 128,          # åˆ†å—å¤§å°
    "delimiter": "\n!?ã€‚ï¼›ï¼ï¼Ÿ",      # åˆ†éš”ç¬¦
    "layout_recognize": true,        # ç‰ˆé¢è¯†åˆ«
    "table_context_size": 0,         # è¡¨æ ¼ä¸Šä¸‹æ–‡
    "image_context_size": 0,         # å›¾ç‰‡ä¸Šä¸‹æ–‡
    "raptor": false,                 # RAPTOR åˆ†å±‚æ£€ç´¢
    "knowledge_graph": false,        # çŸ¥è¯†å›¾è°±
    "pages": [[1, 1000000]],         # å¤„ç†é¡µç èŒƒå›´
    "task_page_size": 12,            # ä»»åŠ¡é¡µé¢å¤§å°
    "llm_id": "..."                  # LLM æ¨¡å‹ ID
}
```

**æ­¥éª¤ 4: æ•°æ®åº“æŒä¹…åŒ–**
```python
# ä¿å­˜åˆ°æ•°æ®åº“
if not KnowledgebaseService.save(**res):
    return get_data_error_result()

return get_json_result(data={"kb_id": res["id"]})
```

#### å…³é”®ä»£ç ä½ç½®

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | è¡Œå· | è¯´æ˜ |
|------|---------|------|------|
| API ç«¯ç‚¹ | [api/apps/kb_app.py](api/apps/kb_app.py#L47-L67) | 47-67 | create() å‡½æ•° |
| æœåŠ¡å±‚ | [api/db/services/knowledgebase_service.py](api/db/services/knowledgebase_service.py#L373-L430) | 373-430 | create_with_name() |
| æ•°æ®æ¨¡å‹ | [api/db/db_models.py](api/db/db_models.py#L734-L768) | 734-768 | Knowledgebase ç±» |

---

### 2.2 çŸ¥è¯†åº“æ›´æ–°æµç¨‹

#### æµç¨‹å›¾

![çŸ¥è¯†åº“æ›´æ–°æµç¨‹](./05-update-kb-sequence.puml)

#### å¯æ›´æ–°å­—æ®µ

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | é™åˆ¶ |
|------|------|------|------|
| `name` | String | çŸ¥è¯†åº“åç§° | å¿…å¡«ï¼Œâ‰¤1024 å­—èŠ‚ |
| `description` | Text | æè¿°ä¿¡æ¯ | å¯é€‰ |
| `parser_id` | String | è§£æå™¨ ID | naive/paper/book/... |
| `parser_config` | JSON | è§£æé…ç½® | è¯¦è§è§£æé…ç½®ç« èŠ‚ |
| `language` | String | è¯­è¨€ | Chinese/English |
| `embd_id` | String | åµŒå…¥æ¨¡å‹ | text-embedding-3-large |
| `similarity_threshold` | Float | ç›¸ä¼¼åº¦é˜ˆå€¼ | 0.0-1.0 |
| `vector_similarity_weight` | Float | å‘é‡æƒé‡ | 0.0-1.0 |
| `permission` | String | æƒé™ | me/team |
| `pagerank` | Integer | PageRank å€¼ | â‰¥0 |
| `avatar` | Text | å›¾æ ‡ Base64 | å¯é€‰ |

#### ç‰¹æ®Šå¤„ç†: PageRank æ›´æ–°

PageRank æ˜¯çŸ¥è¯†åº“åœ¨æ£€ç´¢ä¸­çš„æƒé‡ï¼Œæ›´æ–°æ—¶éœ€è¦åŒæ­¥åˆ° Elasticsearch:

```python
# å¦‚æœ PageRank å‘ç”Ÿå˜åŒ–
if kb.pagerank != req.get("pagerank", 0):
    if req.get("pagerank", 0) > 0:
        # æ›´æ–°æ‰€æœ‰åˆ†å—çš„ PageRank å­—æ®µ
        await asyncio.to_thread(
            settings.docStoreConn.update,
            {"kb_id": kb.id},
            {PAGERANK_FLD: req["pagerank"]},
            search.index_name(kb.tenant_id),
            kb.id,
        )
    else:
        # åˆ é™¤ PageRank å­—æ®µ (ES è¦æ±‚éé›¶)
        await asyncio.to_thread(
            settings.docStoreConn.update,
            {"exists": PAGERANK_FLD},
            {"remove": PAGERANK_FLD},
            search.index_name(kb.tenant_id),
            kb.id,
        )
```

---

### 2.3 çŸ¥è¯†åº“æŸ¥è¯¢æµç¨‹

#### åˆ—è¡¨æŸ¥è¯¢

```python
# æ–‡ä»¶: api/apps/kb_app.py
@manager.route('/list', methods=['POST'])
@login_required
async def list_kbs():
    # æŸ¥è¯¢å‚æ•°
    keywords = args.get("keywords", "")      # å…³é”®è¯æœç´¢
    page_number = int(args.get("page", 0))   # é¡µç 
    items_per_page = int(args.get("page_size", 0))
    parser_id = args.get("parser_id")        # æŒ‰è§£æå™¨ç­›é€‰
    orderby = args.get("orderby", "create_time")  # æ’åºå­—æ®µ
    desc = args.get("desc", "true")          # é™åº/å‡åº
    
    # è·å–ç”¨æˆ·æ‰€å±ç§Ÿæˆ·
    tenants = TenantService.get_joined_tenants_by_user_id(current_user.id)
    tenants = [m["tenant_id"] for m in tenants]
    
    # æŸ¥è¯¢çŸ¥è¯†åº“
    kbs, total = KnowledgebaseService.get_by_tenant_ids(
        tenants, current_user.id, page_number,
        items_per_page, orderby, desc, keywords, parser_id
    )
    
    return get_json_result(data={"kbs": kbs, "total": total})
```

#### æŸ¥è¯¢æ¡ä»¶æ„å»º

```python
# æ–‡ä»¶: api/db/services/knowledgebase_service.py
def get_by_tenant_ids(cls, tenant_ids, user_id, page, size, orderby, desc, keywords, parser_id):
    # æ„å»ºåŸºç¡€æŸ¥è¯¢
    kbs = cls.model.select()
    
    # 1. ç§Ÿæˆ·è¿‡æ»¤
    kbs = kbs.where(cls.model.tenant_id.in_(tenant_ids))
    
    # 2. çŠ¶æ€è¿‡æ»¤ (ä»…æœ‰æ•ˆæ•°æ®)
    kbs = kbs.where(cls.model.status == StatusEnum.VALID.value)
    
    # 3. æƒé™è¿‡æ»¤
    # 'me': ä»…åˆ›å»ºè€…å¯è§
    # 'team': å›¢é˜Ÿæˆå‘˜å¯è§
    kbs = kbs.where(
        (cls.model.permission == PermissionEnum.TEAM.value) |
        (cls.model.created_by == user_id)
    )
    
    # 4. å…³é”®è¯æœç´¢ (æ¨¡ç³ŠåŒ¹é…åç§°æˆ–æè¿°)
    if keywords:
        kbs = kbs.where(
            (cls.model.name.contains(keywords)) |
            (cls.model.description.contains(keywords))
        )
    
    # 5. è§£æå™¨ç­›é€‰
    if parser_id:
        kbs = kbs.where(cls.model.parser_id == parser_id)
    
    # 6. æ’åº
    if desc:
        kbs = kbs.order_by(cls.model.__getattribute__(cls.model, orderby).desc())
    else:
        kbs = kbs.order_by(cls.model.__getattribute__(cls.model, orderby).asc())
    
    # 7. åˆ†é¡µ
    if page > 0 and size > 0:
        kbs = kbs.paginate(page, size)
    
    return kbs, total_count
```

#### è¯¦æƒ…æŸ¥è¯¢

```python
# æ–‡ä»¶: api/apps/kb_app.py
@manager.route('/detail', methods=['GET'])
@login_required
@validate_request("kb_id")
async def detail():
    kb_id = request.args.get("kb_id")
    
    # æƒé™æ£€æŸ¥
    if not KnowledgebaseService.accessible4deletion(kb_id, current_user.id):
        return get_json_result(data=False, message='No authorization.')
    
    # è·å–è¯¦æƒ…
    kb = KnowledgebaseService.get_detail(kb_id)
    
    return get_json_result(data=kb)
```

---

### 2.4 çŸ¥è¯†åº“åˆ é™¤æµç¨‹

#### æµç¨‹å›¾

![çŸ¥è¯†åº“åˆ é™¤æµç¨‹](./06-delete-kb-sequence.puml)

#### åˆ é™¤æ­¥éª¤è¯¦è§£
```python
for kb in kbs:
    # 1. åˆ é™¤ ES ä¸­çš„åˆ†å—æ•°æ®
    settings.docStoreConn.delete(
        {"kb_id": kb.id}, 
        search.index_name(kb.tenant_id), 
        kb.id
    )
    
    # 2. åˆ é™¤ ES ç´¢å¼•
    settings.docStoreConn.deleteIdx(
        search.index_name(kb.tenant_id), 
        kb.id
    )
    
    # 3. åˆ é™¤ MinIO å­˜å‚¨æ¡¶
    if hasattr(settings.STORAGE_IMPL, 'remove_bucket'):
        settings.STORAGE_IMPL.remove_bucket(kb.id)
```

#### åˆ é™¤ä¿æŠ¤æœºåˆ¶

- âœ… **æƒé™æ£€æŸ¥**: ä»…åˆ›å»ºè€…å¯åˆ é™¤
- âœ… **çº§è”åˆ é™¤**: è‡ªåŠ¨åˆ é™¤æ–‡æ¡£ã€åˆ†å—ã€ä»»åŠ¡
- âœ… **å¼‚æ­¥æ‰§è¡Œ**: ä½¿ç”¨ `asyncio.to_thread()` é¿å…é˜»å¡
- âœ… **äº‹åŠ¡ä¿æŠ¤**: æ•°æ®åº“æ“ä½œåœ¨äº‹åŠ¡ä¸­æ‰§è¡Œ
- âš ï¸ **ä¸å¯æ¢å¤**: åˆ é™¤æ“ä½œæ— æ³•æ’¤é”€

---

### 2.5 æ–‡æ¡£ä¸Šä¼ æµç¨‹

#### æ–‡æ¡£ä¸Šä¼ æµç¨‹å›¾

![æ–‡æ¡£ä¸Šä¼ æµç¨‹](./02-upload-document-sequence.puml)

#### æ–‡æ¡£è§£ææµç¨‹å›¾
    
![æ–‡æ¡£è§£ææµç¨‹](./03-parse-document-sequence.puml)

### 3.1 Knowledgebase æ¨¡å‹

#### å­—æ®µè¯´æ˜

```python
class Knowledgebase(DataBaseModel):
    """çŸ¥è¯†åº“æ•°æ®æ¨¡å‹"""
    
    # === åŸºæœ¬ä¿¡æ¯ ===
    id = CharField(max_length=32, primary_key=True)
    # UUID æ ¼å¼ï¼Œä¾‹å¦‚: "kb_a1b2c3d4e5f6"
    
    tenant_id = CharField(max_length=32, null=False, index=True)
    # ç§Ÿæˆ· IDï¼Œç”¨äºå¤šç§Ÿæˆ·éš”ç¦»
    
    created_by = CharField(max_length=32, null=False, index=True)
    # åˆ›å»ºè€… ID (é€šå¸¸ç­‰äº tenant_id)
    
    name = CharField(max_length=128, null=False, index=True)
    # çŸ¥è¯†åº“åç§°ï¼Œæœ€å¤§ 1024 å­—èŠ‚ (UTF-8)
    
    description = TextField(null=True)
    # æè¿°ä¿¡æ¯ï¼Œå¯Œæ–‡æœ¬
    
    avatar = TextField(null=True)
    # å›¾æ ‡ï¼ŒBase64 ç¼–ç 
    
    # === é…ç½®ä¿¡æ¯ ===
    language = CharField(max_length=32, default="Chinese", index=True)
    # è¯­è¨€: Chinese | English | Japanese | ...
    
    embd_id = CharField(max_length=128, null=False, index=True)
    # åµŒå…¥æ¨¡å‹ ID
    # ä¾‹å¦‚: "text-embedding-3-large", "bge-large-zh-v1.5"
    
    parser_id = CharField(max_length=32, default="naive", index=True)
    # è§£æå™¨ ID: naive | paper | book | resume | qa | ...
    
    parser_config = JSONField(default={...})
    # è§£æé…ç½® (è¯¦è§ä¸‹æ–‡)
    
    pipeline_id = CharField(max_length=32, null=True, index=True)
    # æµæ°´çº¿ ID (æœªæ¥åŠŸèƒ½)
    
    # === æƒé™é…ç½® ===
    permission = CharField(max_length=16, default="me", index=True)
    # æƒé™: me (ä»…è‡ªå·±) | team (å›¢é˜Ÿå…±äº«)
    
    # === ç»Ÿè®¡ä¿¡æ¯ ===
    doc_num = IntegerField(default=0, index=True)
    # æ–‡æ¡£æ•°é‡
    
    chunk_num = IntegerField(default=0, index=True)
    # åˆ†å—æ•°é‡
    
    token_num = IntegerField(default=0, index=True)
    # Token æ€»æ•°
    
    # === æ£€ç´¢é…ç½® ===
    similarity_threshold = FloatField(default=0.2, index=True)
    # ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0 - 1.0)
    # ä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤
    
    vector_similarity_weight = FloatField(default=0.3, index=True)
    # å‘é‡ç›¸ä¼¼åº¦æƒé‡ (0.0 - 1.0)
    # æ··åˆæ£€ç´¢æ—¶çš„æƒé‡åˆ†é…
    
    pagerank = IntegerField(default=0, index=False)
    # PageRank å€¼ï¼Œç”¨äºç»“æœæ’åº
    
    # === é«˜çº§åŠŸèƒ½ä»»åŠ¡ ===
    graphrag_task_id = CharField(max_length=32, null=True, index=True)
    graphrag_task_finish_at = DateTimeField(null=True)
    # çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡
    
    raptor_task_id = CharField(max_length=32, null=True, index=True)
    raptor_task_finish_at = DateTimeField(null=True)
    # RAPTOR åˆ†å±‚æ£€ç´¢ä»»åŠ¡
    
    mindmap_task_id = CharField(max_length=32, null=True, index=True)
    mindmap_task_finish_at = DateTimeField(null=True)
    # æ€ç»´å¯¼å›¾ç”Ÿæˆä»»åŠ¡
    
    # === çŠ¶æ€å­—æ®µ ===
    status = CharField(max_length=1, default="1", index=True)
    # 0: å·²åˆ é™¤ (è½¯åˆ é™¤)
    # 1: æœ‰æ•ˆ
    
    # === æ—¶é—´æˆ³ ===
    create_time = DateTimeField(auto_now_add=True)
    update_time = DateTimeField(auto_now=True)
```

#### ç´¢å¼•ç­–ç•¥

```sql
-- å¤åˆç´¢å¼•
CREATE INDEX idx_tenant_status ON knowledgebase(tenant_id, status);
CREATE INDEX idx_tenant_created ON knowledgebase(tenant_id, created_by);
CREATE INDEX idx_tenant_permission ON knowledgebase(tenant_id, permission);

-- æ’åºç´¢å¼•
CREATE INDEX idx_create_time_desc ON knowledgebase(create_time DESC);
CREATE INDEX idx_doc_num_desc ON knowledgebase(doc_num DESC);
```

---

### 3.2 Document æ¨¡å‹

#### å­—æ®µè¯´æ˜

```python
class Document(DataBaseModel):
    """æ–‡æ¡£æ•°æ®æ¨¡å‹"""
    
    # === åŸºæœ¬ä¿¡æ¯ ===
    id = CharField(max_length=32, primary_key=True)
    # æ–‡æ¡£ ID
    
    kb_id = CharField(max_length=256, null=False, index=True)
    # æ‰€å±çŸ¥è¯†åº“ ID
    
    name = CharField(max_length=255, null=True, index=True)
    # æ–‡ä»¶å (ä¸å«è·¯å¾„)
    
    type = CharField(max_length=32, null=False, index=True)
    # æ–‡ä»¶ç±»å‹: pdf | docx | xlsx | ...
    
    suffix = CharField(max_length=32, null=False, index=True)
    # çœŸå®æ–‡ä»¶æ‰©å±•å
    
    size = IntegerField(default=0, index=True)
    # æ–‡ä»¶å¤§å° (å­—èŠ‚)
    
    location = CharField(max_length=255, null=True, index=True)
    # MinIO å­˜å‚¨è·¯å¾„
    
    thumbnail = TextField(null=True)
    # ç¼©ç•¥å›¾ Base64
    
    # === è§£æé…ç½® ===
    parser_id = CharField(max_length=32, null=False, index=True)
    # è§£æå™¨ ID (ç»§æ‰¿è‡ª KB æˆ–å•ç‹¬é…ç½®)
    
    parser_config = JSONField(default={...})
    # è§£æé…ç½® (ç»§æ‰¿è‡ª KB æˆ–å•ç‹¬é…ç½®)
    
    pipeline_id = CharField(max_length=32, null=True, index=True)
    # æµæ°´çº¿ ID
    
    # === è§£æçŠ¶æ€ ===
    run = CharField(max_length=1, default="0", index=True)
    # 0: NEW     - æœªå¼€å§‹
    # 1: RUNNING - è§£æä¸­
    # 2: DONE    - å®Œæˆ
    # 3: CANCEL  - å–æ¶ˆ
    # 4: FAIL    - å¤±è´¥
    
    progress = FloatField(default=0, index=True)
    # è§£æè¿›åº¦ (0.0 - 1.0)
    
    progress_msg = TextField(default="")
    # è¿›åº¦æ¶ˆæ¯
    
    process_begin_at = DateTimeField(null=True, index=True)
    # è§£æå¼€å§‹æ—¶é—´
    
    process_duration = FloatField(default=0)
    # è§£æè€—æ—¶ (ç§’)
    
    # === ç»Ÿè®¡ä¿¡æ¯ ===
    chunk_num = IntegerField(default=0, index=True)
    # åˆ†å—æ•°é‡
    
    token_num = IntegerField(default=0, index=True)
    # Token æ•°é‡
    
    # === å…ƒæ•°æ® ===
    source_type = CharField(max_length=128, default="local", index=True)
    # æ¥æºç±»å‹: local | s3 | web | database | api
    
    meta_fields = JSONField(default={})
    # è‡ªå®šä¹‰å…ƒæ•°æ®å­—æ®µ
    
    created_by = CharField(max_length=32, null=False, index=True)
    # åˆ›å»ºè€… ID
    
    # === çŠ¶æ€å­—æ®µ ===
    status = CharField(max_length=1, default="1", index=True)
    # 0: å·²åˆ é™¤
    # 1: æœ‰æ•ˆ
    
    # === æ—¶é—´æˆ³ ===
    create_time = DateTimeField(auto_now_add=True)
    update_time = DateTimeField(auto_now=True)
```

---

### 3.3 Chunk æ•°æ®ç»“æ„ (Elasticsearch)

#### ç´¢å¼• Mapping

```json
{
  "mappings": {
    "properties": {
      "id": {"type": "keyword"},
      "kb_id": {"type": "keyword"},
      "doc_id": {"type": "keyword"},
      "content": {
        "type": "text",
        "analyzer": "ik_max_word",
        "search_analyzer": "ik_smart"
      },
      "content_with_weight": {
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "embedding": {
        "type": "dense_vector",
        "dims": 1536,
        "index": true,
        "similarity": "cosine"
      },
      "important_keywords": {
        "type": "keyword"
      },
      "img_id": {"type": "keyword"},
      "page_num": {"type": "integer"},
      "position": {"type": "object"},
      "create_time": {"type": "date"}
    }
  }
}
```

#### å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `id` | Keyword | åˆ†å— ID (UUID) |
| `kb_id` | Keyword | çŸ¥è¯†åº“ ID |
| `doc_id` | Keyword | æ–‡æ¡£ ID |
| `content` | Text | åˆ†å—æ–‡æœ¬å†…å®¹ (åˆ†è¯ç´¢å¼•) |
| `content_with_weight` | Text | å¸¦å…³é”®è¯æƒé‡çš„å†…å®¹ |
| `embedding` | Dense Vector | å‘é‡åµŒå…¥ (1536/3072ç»´) |
| `important_keywords` | Keyword[] | é‡è¦å…³é”®è¯åˆ—è¡¨ |
| `img_id` | Keyword | å…³è”å›¾ç‰‡ ID |
| `page_num` | Integer | é¡µç  |
| `position` | Object | é¡µé¢ä½ç½® {x, y, w, h} |
| `create_time` | Date | åˆ›å»ºæ—¶é—´ |

---

## 4. API æ¥å£å®ç°

### 4.1 RESTful API è®¾è®¡

#### API è·¯ç”±è¡¨

| æ–¹æ³• | è·¯å¾„ | åŠŸèƒ½ | æƒé™ |
|------|------|------|------|
| POST | `/api/v1/kb/create` | åˆ›å»ºçŸ¥è¯†åº“ | ç™»å½•ç”¨æˆ· |
| POST | `/api/v1/kb/update` | æ›´æ–°çŸ¥è¯†åº“ | åˆ›å»ºè€… |
| POST | `/api/v1/kb/list` | æŸ¥è¯¢çŸ¥è¯†åº“åˆ—è¡¨ | ç™»å½•ç”¨æˆ· |
| GET | `/api/v1/kb/detail` | è·å–çŸ¥è¯†åº“è¯¦æƒ… | æœ‰æƒé™ç”¨æˆ· |
| POST | `/api/v1/kb/rm` | åˆ é™¤çŸ¥è¯†åº“ | åˆ›å»ºè€… |
| POST | `/api/v1/document/upload` | ä¸Šä¼ æ–‡æ¡£ | æœ‰æƒé™ç”¨æˆ· |
| POST | `/api/v1/document/run` | è§£ææ–‡æ¡£ | æœ‰æƒé™ç”¨æˆ· |
| POST | `/api/v1/document/list` | æ–‡æ¡£åˆ—è¡¨ | æœ‰æƒé™ç”¨æˆ· |
| POST | `/api/v1/document/rm` | åˆ é™¤æ–‡æ¡£ | æœ‰æƒé™ç”¨æˆ· |
| POST | `/api/v1/kb/search` | æ£€ç´¢ | æœ‰æƒé™ç”¨æˆ· |

#### ç»Ÿä¸€å“åº”æ ¼å¼

```json
{
  "code": 0,           // çŠ¶æ€ç : 0=æˆåŠŸ, å…¶ä»–=é”™è¯¯
  "data": {...},       // å“åº”æ•°æ®
  "message": "OK"      // æ¶ˆæ¯
}
```

#### é”™è¯¯ç å®šä¹‰

```python
class RetCode:
    SUCCESS = 0                    # æˆåŠŸ
    ARGUMENT_ERROR = 101           # å‚æ•°é”™è¯¯
    DATA_ERROR = 102               # æ•°æ®é”™è¯¯
    OPERATING_ERROR = 103          # æ“ä½œé”™è¯¯
    AUTHENTICATION_ERROR = 104     # è®¤è¯é”™è¯¯
    AUTHORIZATION_ERROR = 105      # æˆæƒé”™è¯¯
    SERVER_ERROR = 500             # æœåŠ¡å™¨é”™è¯¯
    DATABASE_ERROR = 501           # æ•°æ®åº“é”™è¯¯
```

---

### 4.2 è¯·æ±‚éªŒè¯è£…é¥°å™¨

```python
def validate_request(*required_fields):
    """éªŒè¯è¯·æ±‚å‚æ•°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            req = await get_request_json()
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            for field in required_fields:
                if field not in req:
                    return get_json_result(
                        data=False,
                        message=f'Missing required field: {field}',
                        code=RetCode.ARGUMENT_ERROR
                    )
            
            return await func(*args, **kwargs)
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@manager.route('/create', methods=['post'])
@login_required
@validate_request("name")
async def create():
    ...
```

---

### 4.3 æƒé™æ§åˆ¶è£…é¥°å™¨

```python
def check_kb_team_permission(kb, user_id):
    """æ£€æŸ¥çŸ¥è¯†åº“æƒé™"""
    # åˆ›å»ºè€…å§‹ç»ˆæœ‰æƒé™
    if kb.created_by == user_id:
        return True
    
    # å›¢é˜Ÿæƒé™æ£€æŸ¥
    if kb.permission == PermissionEnum.TEAM.value:
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦åœ¨åŒä¸€ç§Ÿæˆ·
        return kb.tenant_id == user_id
    
    return False
```

---

## 5. æœåŠ¡å±‚æ¶æ„

### 5.1 KnowledgebaseService æœåŠ¡å±‚

#### æœåŠ¡ç±»ç»§æ‰¿å…³ç³»

```mermaid
classDiagram
    class CommonService {
        +model
        +query()
        +get_by_id()
        +save()
        +update_by_id()
        +delete_by_id()
    }
    
    class KnowledgebaseService {
        +model = Knowledgebase
        +accessible4deletion()
        +is_parsed_done()
        +create_with_name()
        +get_by_tenant_ids()
        +get_detail()
        +update_parser_config()
    }
    
    CommonService <|-- KnowledgebaseService
```

#### æ ¸å¿ƒæ–¹æ³•è¯´æ˜

**1. accessible4deletion(kb_id, user_id)**
```python
@classmethod
@DB.connection_context()
def accessible4deletion(cls, kb_id, user_id):
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æƒé™åˆ é™¤çŸ¥è¯†åº“
    
    Args:
        kb_id: çŸ¥è¯†åº“ ID
        user_id: ç”¨æˆ· ID
    
    Returns:
        bool: True=æœ‰æƒé™, False=æ— æƒé™
    """
    # ä»…åˆ›å»ºè€…å¯åˆ é™¤
    kbs = cls.query(created_by=user_id, id=kb_id)
    return len(kbs) > 0
```

**2. is_parsed_done(kb_id)**
```python
@classmethod
def is_parsed_done(cls, kb_id):
    """æ£€æŸ¥çŸ¥è¯†åº“ä¸‹æ‰€æœ‰æ–‡æ¡£æ˜¯å¦è§£æå®Œæˆ
    
    Returns:
        tuple: (bool, progress_msg)
        - True: å…¨éƒ¨å®Œæˆ
        - False: ä»åœ¨è¿›è¡Œä¸­
    """
    # æŸ¥è¯¢è§£æä¸­çš„æ–‡æ¡£
    docs = DocumentService.query(
        kb_id=kb_id,
        run=TaskStatus.RUNNING.value
    )
    
    if docs:
        return False, f"Still {len(docs)} documents parsing..."
    
    # æ£€æŸ¥å¤±è´¥çš„æ–‡æ¡£
    failed_docs = DocumentService.query(
        kb_id=kb_id,
        run=TaskStatus.FAIL.value
    )
    
    if failed_docs:
        return False, f"{len(failed_docs)} documents failed"
    
    return True, "All documents parsed"
```

**3. get_by_tenant_ids()**
```python
@classmethod
def get_by_tenant_ids(cls, tenant_ids, user_id, page, size, 
                       orderby, desc, keywords, parser_id):
    """æŒ‰ç§Ÿæˆ·æŸ¥è¯¢çŸ¥è¯†åº“åˆ—è¡¨
    
    æ”¯æŒ:
    - å¤šç§Ÿæˆ·è¿‡æ»¤
    - æƒé™è¿‡æ»¤ (me/team)
    - å…³é”®è¯æœç´¢
    - è§£æå™¨ç­›é€‰
    - åˆ†é¡µæ’åº
    
    Returns:
        tuple: (kbs_list, total_count)
    """
    # å®ç°è§å‰æ–‡ 2.3 èŠ‚
```

**4. create_with_name()**
```python
@classmethod
@DB.connection_context()
def create_with_name(cls, *, name, tenant_id, parser_id=None, **kwargs):
    """åˆ›å»ºçŸ¥è¯†åº“
    
    åŒ…å«å®Œæ•´çš„éªŒè¯é€»è¾‘:
    1. åç§°éªŒè¯ (ç±»å‹ã€é•¿åº¦ã€éç©º)
    2. åç§°å»é‡
    3. ç§Ÿæˆ·éªŒè¯
    4. è§£æé…ç½®æ„å»º
    
    Returns:
        tuple: (success: bool, data_or_error)
    """
    # å®ç°è§å‰æ–‡ 2.1 èŠ‚
```

**5. update_parser_config(kb_id, config)**
```python
@classmethod
def update_parser_config(cls, kb_id, config):
    """æ›´æ–°çŸ¥è¯†åº“çš„è§£æé…ç½®
    
    æ³¨æ„:
    - ä»…æ›´æ–°æä¾›çš„é…ç½®é¡¹
    - ä¿ç•™æœªæä¾›çš„é…ç½®é¡¹
    - éªŒè¯é…ç½®æ ¼å¼
    """
    e, kb = cls.get_by_id(kb_id)
    if not e:
        return False
    
    # åˆå¹¶é…ç½®
    new_config = {**kb.parser_config, **config}
    
    return cls.update_by_id(kb_id, {"parser_config": new_config})
```

---

### 5.2 DocumentService æœåŠ¡å±‚

#### æ ¸å¿ƒæ–¹æ³•

**1. upload_document(kb, file_objs, user_id)**
```python
def upload_document(kb, file_objs, user_id):
    """ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
    
    å¤„ç†æµç¨‹:
    1. ç”Ÿæˆæ–‡ä»¶ ID
    2. ä¸Šä¼ åˆ° MinIO
    3. åˆ›å»º File è®°å½•
    4. åˆ›å»º Document è®°å½•
    5. åˆ›å»º File2Document å…³è”
    
    Returns:
        tuple: (errors, files)
    """
    errors = []
    files = []
    
    for file_obj in file_objs:
        try:
            # 1. ç”Ÿæˆ ID
            file_id = get_uuid()
            doc_id = get_uuid()
            
            # 2. ä¸Šä¼ åˆ° MinIO
            location = f"{kb.id}/{file_id}/{file_obj.filename}"
            STORAGE.put(location, file_obj.read())
            
            # 3. åˆ›å»º File è®°å½•
            file_record = File.create(
                id=file_id,
                name=file_obj.filename,
                size=file_obj.content_length,
                location=location,
                type=get_file_type(file_obj.filename),
                source_type=FileSource.KNOWLEDGEBASE
            )
            
            # 4. åˆ›å»º Document è®°å½•
            doc_record = Document.create(
                id=doc_id,
                kb_id=kb.id,
                name=file_obj.filename,
                location=location,
                size=file_obj.content_length,
                type=get_file_type(file_obj.filename),
                parser_id=kb.parser_id,
                parser_config=kb.parser_config,
                created_by=user_id
            )
            
            # 5. åˆ›å»ºå…³è”
            File2Document.create(
                file_id=file_id,
                document_id=doc_id
            )
            
            files.append((doc_record.to_dict(), None))
            
        except Exception as e:
            errors.append(str(e))
    
    return errors, files
```

**2. run(tenant_id, doc_dict, kb_table_num_map)**
```python
@classmethod
def run(cls, tenant_id, doc_dict, kb_table_num_map):
    """å¯åŠ¨æ–‡æ¡£è§£æä»»åŠ¡
    
    Args:
        tenant_id: ç§Ÿæˆ· ID
        doc_dict: æ–‡æ¡£æ•°æ®å­—å…¸
        kb_table_num_map: çŸ¥è¯†åº“è¡¨æ•°é‡æ˜ å°„ (ç”¨äºç»Ÿè®¡)
    """
    # 1. è·å–åµŒå…¥æ¨¡å‹
    e, kb = KnowledgebaseService.get_by_id(doc_dict["kb_id"])
    embd_mdl = LLMBundle(kb.tenant_id, LLMType.EMBEDDING, kb.embd_id)
    
    # 2. åˆ›å»ºè§£æä»»åŠ¡
    task = Task.create(
        id=get_uuid(),
        doc_id=doc_dict["id"],
        from_page=0,
        to_page=-1,  # å…¨éƒ¨é¡µé¢
        progress=0,
        progress_msg=""
    )
    
    # 3. æäº¤åˆ°ä»»åŠ¡é˜Ÿåˆ—
    task_broker.queue_tasks([{
        "doc_id": doc_dict["id"],
        "tenant_id": tenant_id,
        "parser_config": doc_dict["parser_config"],
        "embd_mdl": embd_mdl
    }])
```

**3. remove_document(doc, tenant_id)**
```python
@classmethod
def remove_document(cls, doc, tenant_id):
    """åˆ é™¤æ–‡æ¡£åŠå…¶å…³è”æ•°æ®
    
    åˆ é™¤å†…å®¹:
    1. æ–‡æ¡£è®°å½•
    2. åˆ†å—æ•°æ® (ES)
    3. ä»»åŠ¡è®°å½•
    4. æ–‡ä»¶è®°å½•
    5. MinIO å¯¹è±¡
    """
    try:
        # 1. åˆ é™¤ ES ä¸­çš„åˆ†å—
        settings.docStoreConn.delete(
            {"doc_id": doc.id},
            search.index_name(tenant_id),
            doc.kb_id
        )
        
        # 2. åˆ é™¤ä»»åŠ¡
        TaskService.filter_delete([Task.doc_id == doc.id])
        
        # 3. åˆ é™¤æ–‡æ¡£è®°å½•
        cls.delete_by_id(doc.id)
        
        # 4. åˆ é™¤æ–‡ä»¶
        f2d = File2DocumentService.get_by_document_id(doc.id)
        if f2d:
            FileService.delete_by_id(f2d[0].file_id)
            settings.STORAGE_IMPL.rm(doc.location)
        
        return True
    except Exception as e:
        logging.error(f"Remove document error: {e}")
        return False
```

---

## 6. è§£æé…ç½®è¯¦è§£ (parser_config)

### 6.1 é…ç½®é¡¹è¯´æ˜

#### å®Œæ•´é…ç½®ç»“æ„

```json
{
  "chunk_token_num": 128,
  "delimiter": "\n!?ã€‚ï¼›ï¼ï¼Ÿ",
  "layout_recognize": true,
  "table_context_size": 0,
  "image_context_size": 0,
  "raptor": false,
  "raptor_depth": 2,
  "knowledge_graph": false,
  "pages": [[1, 1000000]],
  "task_page_size": 12,
  "llm_id": "gpt-4o-mini",
  "filename_embd_weight": 0.1,
  "auto_keywords": true,
  "auto_questions": false
}
```

#### é…ç½®é¡¹è¯¦è§£

| é…ç½®é¡¹ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| **chunk_token_num** | Integer | 128 | åˆ†å—å¤§å°ï¼ˆToken æ•°ï¼‰<br/>- å¤ªå°: ä¸Šä¸‹æ–‡ä¸è¶³<br/>- å¤ªå¤§: ç²¾ç¡®åº¦ä¸‹é™<br/>- å»ºè®®: 128-512 |
| **delimiter** | String | `"\n!?ã€‚ï¼›ï¼ï¼Ÿ"` | åˆ†å—åˆ†éš”ç¬¦<br/>- ä¸­æ–‡: ã€‚ï¼ï¼Ÿ<br/>- è‹±æ–‡: .!?<br/>- æ¢è¡Œ: \n |
| **layout_recognize** | Boolean | true | ç‰ˆé¢è¯†åˆ«<br/>- true: è¯†åˆ«æ ‡é¢˜ã€æ®µè½ã€è¡¨æ ¼<br/>- false: çº¯æ–‡æœ¬æå– |
| **table_context_size** | Integer | 0 | è¡¨æ ¼ä¸Šä¸‹æ–‡è¡Œæ•°<br/>- 0: ä»…è¡¨æ ¼å†…å®¹<br/>- >0: åŒ…å«å‰å N è¡Œ |
| **image_context_size** | Integer | 0 | å›¾ç‰‡ä¸Šä¸‹æ–‡è¡Œæ•°<br/>- 0: ä»…å›¾ç‰‡æè¿°<br/>- >0: åŒ…å«å‰å N è¡Œ |
| **raptor** | Boolean | false | å¯ç”¨ RAPTOR åˆ†å±‚æ£€ç´¢<br/>- è‡ªåŠ¨ç”Ÿæˆå¤šå±‚æ¬¡æ‘˜è¦<br/>- æå‡é•¿æ–‡æ¡£æ£€ç´¢æ•ˆæœ |
| **raptor_depth** | Integer | 2 | RAPTOR å±‚çº§æ·±åº¦<br/>- 1: ä»…ä¸€å±‚æ‘˜è¦<br/>- 2-3: å¤šå±‚æ¬¡æ‘˜è¦ |
| **knowledge_graph** | Boolean | false | æ„å»ºçŸ¥è¯†å›¾è°±<br/>- å®ä½“è¯†åˆ«<br/>- å…³ç³»æŠ½å– |
| **pages** | Array | `[[1, 1000000]]` | å¤„ç†é¡µç èŒƒå›´<br/>- `[[1,10]]`: ä»…å‰ 10 é¡µ<br/>- `[[1,5],[10,15]]`: å¤šæ®µ |
| **task_page_size** | Integer | 12 | ä»»åŠ¡å¤„ç†é¡µé¢å¤§å°<br/>- å•ä¸ªä»»åŠ¡å¤„ç†çš„é¡µæ•° |
| **llm_id** | String | auto | LLM æ¨¡å‹ ID<br/>- ç”¨äºæ‘˜è¦ã€é—®é¢˜ç”Ÿæˆç­‰ |
| **filename_embd_weight** | Float | 0.1 | æ–‡ä»¶ååµŒå…¥æƒé‡<br/>- 0: ä¸ä½¿ç”¨æ–‡ä»¶å<br/>- 0.1-0.3: é€‚åº¦æƒé‡ |
| **auto_keywords** | Boolean | true | è‡ªåŠ¨æå–å…³é”®è¯<br/>- æå‡æ£€ç´¢ç›¸å…³æ€§ |
| **auto_questions** | Boolean | false | è‡ªåŠ¨ç”Ÿæˆé—®ç­”å¯¹<br/>- ç”¨äºé—®ç­”è¯„ä¼° |

---

### 6.2 ä¸åŒè§£æå™¨çš„é…ç½®

#### 1. Naive Parser (é€šç”¨è§£æå™¨)

```json
{
  "parser_id": "naive",
  "parser_config": {
    "chunk_token_num": 128,
    "layout_recognize": true
  }
}
```

**é€‚ç”¨åœºæ™¯**: 
- âœ… é€šç”¨æ–‡æ¡£
- âœ… ç®€å• PDF
- âœ… Wordã€TXT æ–‡ä»¶

---

#### 2. Paper Parser (è®ºæ–‡è§£æå™¨)

```json
{
  "parser_id": "paper",
  "parser_config": {
    "chunk_token_num": 256,
    "layout_recognize": true,
    "table_context_size": 3,
    "image_context_size": 2,
    "auto_keywords": true
  }
}
```

**ç‰¹æ€§**:
- è¯†åˆ«è®ºæ–‡ç»“æ„ (æ‘˜è¦ã€å¼•è¨€ã€ç»“è®º)
- è¡¨æ ¼å’Œå›¾è¡¨ä¸Šä¸‹æ–‡å…³è”
- å‚è€ƒæ–‡çŒ®è§£æ
- å…¬å¼è¯†åˆ«

**é€‚ç”¨åœºæ™¯**:
- âœ… å­¦æœ¯è®ºæ–‡
- âœ… ç ”ç©¶æŠ¥å‘Š
- âœ… æŠ€æœ¯æ–‡æ¡£

---

#### 3. Book Parser (ä¹¦ç±è§£æå™¨)

```json
{
  "parser_id": "book",
  "parser_config": {
    "chunk_token_num": 256,
    "layout_recognize": true,
    "raptor": true,
    "raptor_depth": 3
  }
}
```

**ç‰¹æ€§**:
- ç« èŠ‚å±‚çº§è¯†åˆ«
- å¤šå±‚æ¬¡æ‘˜è¦ (RAPTOR)
- ç›®å½•ç”Ÿæˆ
- é•¿æ–‡æ¡£ä¼˜åŒ–

**é€‚ç”¨åœºæ™¯**:
- âœ… é•¿ç¯‡ä¹¦ç±
- âœ… æ•™æ
- âœ… æ‰‹å†Œ

---

#### 4. Resume Parser (ç®€å†è§£æå™¨)

```json
{
  "parser_id": "resume",
  "parser_config": {
    "chunk_token_num": 512,
    "layout_recognize": true,
    "knowledge_graph": true
  }
}
```

**ç‰¹æ€§**:
- ç»“æ„åŒ–å­—æ®µæå– (å§“åã€æ•™è‚²ã€ç»éªŒ)
- å®ä½“è¯†åˆ« (å…¬å¸ã€å­¦æ ¡ã€æŠ€èƒ½)
- çŸ¥è¯†å›¾è°±æ„å»º

**é€‚ç”¨åœºæ™¯**:
- âœ… ä¸ªäººç®€å†
- âœ… æ‹›è˜ç³»ç»Ÿ
- âœ… äººæ‰åº“

---

### 6.3 é…ç½®æœ€ä½³å®è·µ

#### åœºæ™¯ 1: é€šç”¨çŸ¥è¯†åº“

```json
{
  "chunk_token_num": 128,
  "layout_recognize": true,
  "auto_keywords": true
}
```

**ç‰¹ç‚¹**: å¹³è¡¡æ€§èƒ½å’Œè´¨é‡

---

#### åœºæ™¯ 2: é•¿æ–‡æ¡£æ£€ç´¢

```json
{
  "chunk_token_num": 256,
  "raptor": true,
  "raptor_depth": 2,
  "llm_id": "gpt-4o-mini"
}
```

**ç‰¹ç‚¹**: å¤šå±‚æ¬¡æ‘˜è¦ï¼Œæå‡æ£€ç´¢å‡†ç¡®åº¦

---

#### åœºæ™¯ 3: ç»“æ„åŒ–æ–‡æ¡£

```json
{
  "chunk_token_num": 256,
  "layout_recognize": true,
  "table_context_size": 3,
  "knowledge_graph": true
}
```

**ç‰¹ç‚¹**: ä¿ç•™ç»“æ„ä¿¡æ¯ï¼Œæ”¯æŒå›¾è°±æ£€ç´¢

---

## 7. æƒé™æ§åˆ¶æœºåˆ¶

### 7.1 æƒé™æ¨¡å‹

```mermaid
graph TB
    User[ç”¨æˆ·]
    Tenant[ç§Ÿæˆ·]
    KB[çŸ¥è¯†åº“]
    
    User -->|belongs to| Tenant
    Tenant -->|owns| KB
    KB -->|permission| ME[ä»…è‡ªå·±]
    KB -->|permission| TEAM[å›¢é˜Ÿå…±äº«]
    
    ME -->|created_by| Creator[åˆ›å»ºè€…]
    TEAM -->|tenant_id| TeamMembers[å›¢é˜Ÿæˆå‘˜]
```

### 7.2 æƒé™çº§åˆ«

| æƒé™ | å€¼ | å¯è®¿é—®ç”¨æˆ· | è¯´æ˜ |
|------|----|-----------|----|
| **me** | "me" | åˆ›å»ºè€… | ç§æœ‰çŸ¥è¯†åº“ï¼Œä»…åˆ›å»ºè€…å¯è§ |
| **team** | "team" | åŒç§Ÿæˆ·æˆå‘˜ | å›¢é˜Ÿå…±äº«ï¼Œç§Ÿæˆ·å†…æˆå‘˜å¯è§ |

### 7.3 æƒé™æ£€æŸ¥å®ç°

**1. è¯»å–æƒé™æ£€æŸ¥**
```python
def accessible(kb_id, user_id):
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å¯è®¿é—®çŸ¥è¯†åº“"""
    kb = KnowledgebaseService.get_by_id(kb_id)
    
    # åˆ›å»ºè€…å§‹ç»ˆå¯è®¿é—®
    if kb.created_by == user_id:
        return True
    
    # team æƒé™: åŒç§Ÿæˆ·æˆå‘˜å¯è®¿é—®
    if kb.permission == "team":
        user = UserService.get_by_id(user_id)
        return user.tenant_id == kb.tenant_id
    
    return False
```

**2. åˆ é™¤æƒé™æ£€æŸ¥**
```python
def accessible4deletion(kb_id, user_id):
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å¯åˆ é™¤çŸ¥è¯†åº“"""
    # ä»…åˆ›å»ºè€…å¯åˆ é™¤
    kb = KnowledgebaseService.query(
        id=kb_id,
        created_by=user_id
    )
    return len(kb) > 0
```

---

## 8. é”™è¯¯å¤„ç†

### 8.1 å¸¸è§é”™è¯¯ç±»å‹

#### 1. å‚æ•°é”™è¯¯

```python
# é”™è¯¯: ç¼ºå°‘å¿…éœ€å‚æ•°
{
  "code": 101,
  "message": "Missing required field: name",
  "data": False
}

# é”™è¯¯: å‚æ•°ç±»å‹é”™è¯¯
{
  "code": 101,
  "message": "Dataset name must be string.",
  "data": False
}

# é”™è¯¯: å‚æ•°é•¿åº¦è¶…é™
{
  "code": 101,
  "message": "Dataset name length is 1500 which is larger than 1024",
  "data": False
}
```

#### 2. æƒé™é”™è¯¯

```python
# é”™è¯¯: æ— æƒé™
{
  "code": 104,
  "message": "No authorization.",
  "data": False
}

# é”™è¯¯: ä»…åˆ›å»ºè€…å¯æ“ä½œ
{
  "code": 103,
  "message": "Only owner of dataset authorized for this operation.",
  "data": False
}
```

#### 3. æ•°æ®é”™è¯¯

```python
# é”™è¯¯: çŸ¥è¯†åº“ä¸å­˜åœ¨
{
  "code": 102,
  "message": "Can't find this dataset!",
  "data": False
}

# é”™è¯¯: åç§°é‡å¤
{
  "code": 102,
  "message": "Duplicated dataset name.",
  "data": False
}
```

#### 4. æœåŠ¡å™¨é”™è¯¯

```python
# é”™è¯¯: æ•°æ®åº“é”™è¯¯
{
  "code": 500,
  "message": "Database error (Knowledgebase removal)!",
  "data": False
}

# é”™è¯¯: æ–‡ä»¶ä¸Šä¼ å¤±è´¥
{
  "code": 500,
  "message": "Failed to upload file to MinIO",
  "data": False
}
```

---

### 8.2 é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

#### 1. ç»Ÿä¸€é”™è¯¯å¤„ç†

```python
@app.errorhandler(Exception)
async def handle_exception(e):
    """å…¨å±€å¼‚å¸¸å¤„ç†"""
    logging.error(f"Unhandled exception: {e}", exc_info=True)
    
    return get_json_result(
        code=RetCode.SERVER_ERROR,
        message=str(e),
        data=False
    )
```

#### 2. è‡ªå®šä¹‰å¼‚å¸¸

```python
class KBException(Exception):
    """çŸ¥è¯†åº“å¼‚å¸¸åŸºç±»"""
    pass

class KBNotFoundError(KBException):
    """çŸ¥è¯†åº“ä¸å­˜åœ¨"""
    pass

class KBPermissionDeniedError(KBException):
    """çŸ¥è¯†åº“æƒé™æ‹’ç»"""
    pass

# ä½¿ç”¨
try:
    kb = KnowledgebaseService.get_by_id(kb_id)
    if not kb:
        raise KBNotFoundError(f"KB {kb_id} not found")
except KBNotFoundError as e:
    return get_json_result(
        code=RetCode.DATA_ERROR,
        message=str(e),
        data=False
    )
```

#### 3. äº‹åŠ¡å›æ»š

```python
@DB.connection_context()
def create_kb_with_documents(kb_data, doc_files):
    """äº‹åŠ¡åˆ›å»ºçŸ¥è¯†åº“å’Œæ–‡æ¡£"""
    try:
        with DB.atomic():
            # åˆ›å»ºçŸ¥è¯†åº“
            kb = Knowledgebase.create(**kb_data)
            
            # ä¸Šä¼ æ–‡æ¡£
            for file in doc_files:
                doc = Document.create(kb_id=kb.id, ...)
            
            return True, kb
    except Exception as e:
        # è‡ªåŠ¨å›æ»š
        logging.error(f"Transaction failed: {e}")
        return False, str(e)
```

---

## 9. æ€§èƒ½ä¼˜åŒ–

### 9.1 æ•°æ®åº“ä¼˜åŒ–

#### 1. ç´¢å¼•ä¼˜åŒ–

```sql
-- æŸ¥è¯¢ä¼˜åŒ–: æŒ‰ç§Ÿæˆ·+çŠ¶æ€æŸ¥è¯¢
CREATE INDEX idx_tenant_status ON knowledgebase(tenant_id, status);

-- æŸ¥è¯¢ä¼˜åŒ–: æŒ‰ç§Ÿæˆ·+æƒé™æŸ¥è¯¢
CREATE INDEX idx_tenant_permission ON knowledgebase(tenant_id, permission);

-- æ’åºä¼˜åŒ–: æŒ‰åˆ›å»ºæ—¶é—´é™åº
CREATE INDEX idx_create_time_desc ON knowledgebase(create_time DESC);

-- ç»Ÿè®¡ä¼˜åŒ–: æŒ‰æ–‡æ¡£æ•°é‡æ’åº
CREATE INDEX idx_doc_num_desc ON knowledgebase(doc_num DESC);
```

#### 2. æŸ¥è¯¢ä¼˜åŒ–

```python
# æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–
def get_kbs_with_stats(kb_ids):
    """æ‰¹é‡è·å–çŸ¥è¯†åº“åŠç»Ÿè®¡ä¿¡æ¯"""
    # ä½¿ç”¨ IN æŸ¥è¯¢ä»£æ›¿å¾ªç¯
    kbs = Knowledgebase.select().where(
        Knowledgebase.id.in_(kb_ids)
    )
    
    # é¢„åŠ è½½å…³è”æ•°æ®
    kbs = kbs.prefetch(Document, File)
    
    return list(kbs)

# N+1 é—®é¢˜ä¼˜åŒ–
def list_kbs_with_doc_count():
    """åˆ—è¡¨æŸ¥è¯¢æ—¶é¿å… N+1"""
    # ä½¿ç”¨ JOIN æˆ–å­æŸ¥è¯¢
    kbs = Knowledgebase.select(
        Knowledgebase,
        fn.COUNT(Document.id).alias('doc_count')
    ).join(
        Document, JOIN.LEFT_OUTER
    ).group_by(Knowledgebase.id)
    
    return list(kbs)
```

---

### 9.2 ç¼“å­˜ç­–ç•¥

#### 1. Redis ç¼“å­˜

```python
import redis
from functools import wraps

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cache_kb(expire=3600):
    """çŸ¥è¯†åº“ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(kb_id, *args, **kwargs):
            # å°è¯•ä»ç¼“å­˜è·å–
            cache_key = f"kb:{kb_id}"
            cached = redis_client.get(cache_key)
            
            if cached:
                return json.loads(cached)
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
            result = func(kb_id, *args, **kwargs)
            
            # å†™å…¥ç¼“å­˜
            redis_client.setex(
                cache_key,
                expire,
                json.dumps(result)
            )
            
            return result
        return wrapper
    return decorator

@cache_kb(expire=1800)
def get_kb_detail(kb_id):
    """è·å–çŸ¥è¯†åº“è¯¦æƒ…ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    return KnowledgebaseService.get_detail(kb_id)
```

#### 2. ç¼“å­˜å¤±æ•ˆ

```python
def update_kb(kb_id, data):
    """æ›´æ–°çŸ¥è¯†åº“å¹¶æ¸…é™¤ç¼“å­˜"""
    # æ›´æ–°æ•°æ®åº“
    KnowledgebaseService.update_by_id(kb_id, data)
    
    # æ¸…é™¤ç¼“å­˜
    redis_client.delete(f"kb:{kb_id}")
    redis_client.delete(f"kb:list:{user_id}")
```

---

### 9.3 å¼‚æ­¥å¤„ç†

#### 1. å¼‚æ­¥ API

```python
# ä½¿ç”¨ Quart å¼‚æ­¥æ¡†æ¶
@manager.route('/list', methods=['POST'])
@login_required
async def list_kbs():
    """å¼‚æ­¥æŸ¥è¯¢çŸ¥è¯†åº“åˆ—è¡¨"""
    # å¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢
    kbs = await asyncio.to_thread(
        KnowledgebaseService.get_by_tenant_ids,
        tenants, user_id, page, size, orderby, desc, keywords
    )
    
    return get_json_result(data=kbs)
```

#### 2. ä»»åŠ¡é˜Ÿåˆ—

```python
# ä½¿ç”¨ Celery å¤„ç†è€—æ—¶ä»»åŠ¡
@celery.task
def parse_document_task(doc_id):
    """æ–‡æ¡£è§£æä»»åŠ¡ï¼ˆå¼‚æ­¥ï¼‰"""
    doc = DocumentService.get_by_id(doc_id)
    
    # è§£ææ–‡æ¡£
    parser = get_parser(doc.parser_id)
    chunks = parser.parse(doc.location)
    
    # å‘é‡åŒ–
    embeddings = embed_model.encode(chunks)
    
    # å†™å…¥ ES
    docStoreConn.bulk_insert(chunks, embeddings)
    
    # æ›´æ–°çŠ¶æ€
    DocumentService.update_by_id(doc_id, {
        "run": TaskStatus.DONE.value,
        "progress": 1.0
    })
```

---

## 10. æœ€ä½³å®è·µ

### 10.1 çŸ¥è¯†åº“è®¾è®¡

#### 1. å‘½åè§„èŒƒ

```python
# âœ… å¥½çš„å‘½å
"äº§å“æŠ€æœ¯æ–‡æ¡£ 2024"
"å®¢æˆ·æœåŠ¡æ‰‹å†Œ v2.0"
"é”€å”®èµ„æ–™åº“ - åä¸œåŒº"

# âŒ ä¸å¥½çš„å‘½å
"aaa"
"æµ‹è¯•"
"æ–°å»ºçŸ¥è¯†åº“(123)"
```

#### 2. åˆ†ç±»ç­–ç•¥

```python
# æŒ‰ä¸šåŠ¡é¢†åŸŸåˆ†ç±»
knowledge_bases = {
    "æŠ€æœ¯æ–‡æ¡£": ["APIæ–‡æ¡£", "å¼€å‘æŒ‡å—", "æ¶æ„è®¾è®¡"],
    "äº§å“èµ„æ–™": ["äº§å“æ‰‹å†Œ", "ç”¨æˆ·æŒ‡å—", "FAQ"],
    "å†…éƒ¨åŸ¹è®­": ["æ–°å‘˜å·¥åŸ¹è®­", "æŠ€èƒ½æå‡", "ç®¡ç†åŸ¹è®­"]
}

# æŒ‰æ—¶é—´åˆ†ç±»
knowledge_bases_by_time = {
    "2024å¹´": ["Q1èµ„æ–™", "Q2èµ„æ–™", "Q3èµ„æ–™", "Q4èµ„æ–™"],
    "2023å¹´": ["å½’æ¡£èµ„æ–™"]
}
```

#### 3. æƒé™ç®¡ç†

```python
# ç§æœ‰çŸ¥è¯†åº“ (me)
personal_kb = {
    "name": "æˆ‘çš„ä¸ªäººç¬”è®°",
    "permission": "me",
    "description": "ä»…è‡ªå·±å¯è§"
}

# å›¢é˜ŸçŸ¥è¯†åº“ (team)
team_kb = {
    "name": "å›¢é˜Ÿå…±äº«æ–‡æ¡£",
    "permission": "team",
    "description": "å›¢é˜Ÿæˆå‘˜å¯è§"
}
```

---

### 10.2 æ–‡æ¡£ç®¡ç†

#### 1. æ–‡æ¡£ç»„ç»‡

```python
# ä½¿ç”¨æ–‡ä»¶å¤¹ç»“æ„
kb_structure = {
    "æŠ€æœ¯æ–‡æ¡£": {
        "åç«¯": ["API.pdf", "æ•°æ®åº“.pdf"],
        "å‰ç«¯": ["React.pdf", "Vue.pdf"],
        "è¿ç»´": ["Docker.pdf", "K8s.pdf"]
    }
}
```

#### 2. è§£æå™¨é€‰æ‹©

```python
# æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©è§£æå™¨
def choose_parser(filename):
    """æ™ºèƒ½é€‰æ‹©è§£æå™¨"""
    if "è®ºæ–‡" in filename or "paper" in filename.lower():
        return "paper"
    elif "ç®€å†" in filename or "resume" in filename.lower():
        return "resume"
    elif "ä¹¦ç±" in filename or "book" in filename.lower():
        return "book"
    else:
        return "naive"
```

#### 3. æ‰¹é‡å¤„ç†

```python
# æ‰¹é‡ä¸Šä¼ æ–‡æ¡£
def batch_upload(kb_id, files):
    """æ‰¹é‡ä¸Šä¼ å¹¶è§£æ"""
    doc_ids = []
    
    # 1. æ‰¹é‡ä¸Šä¼ 
    for file in files:
        doc = upload_document(kb_id, file)
        doc_ids.append(doc.id)
    
    # 2. æ‰¹é‡è§¦å‘è§£æ
    run_documents(doc_ids, run=TaskStatus.RUNNING.value)
    
    return doc_ids
```

---

### 10.3 æ£€ç´¢ä¼˜åŒ–

#### 1. ç›¸ä¼¼åº¦é˜ˆå€¼è°ƒæ•´

```python
# æ ¹æ®åœºæ™¯è°ƒæ•´é˜ˆå€¼
scenarios = {
    "ç²¾ç¡®åŒ¹é…": {"similarity_threshold": 0.8},  # é«˜é˜ˆå€¼
    "æ¨¡ç³Šæœç´¢": {"similarity_threshold": 0.2},  # ä½é˜ˆå€¼
    "æ¨èç³»ç»Ÿ": {"similarity_threshold": 0.5}   # ä¸­é˜ˆå€¼
}
```

#### 2. æ··åˆæ£€ç´¢æƒé‡

```python
# è°ƒæ•´å‘é‡æ£€ç´¢å’Œå…¨æ–‡æ£€ç´¢çš„æƒé‡
retrieval_config = {
    "vector_similarity_weight": 0.7,  # å‘é‡æ£€ç´¢æƒé‡
    "full_text_weight": 0.3            # å…¨æ–‡æ£€ç´¢æƒé‡
}

# æŠ€æœ¯æ–‡æ¡£: åå‘è¯­ä¹‰ç›¸ä¼¼åº¦
tech_config = {
    "vector_similarity_weight": 0.8,
    "full_text_weight": 0.2
}

# æ³•å¾‹æ–‡æ¡£: åå‘å…³é”®è¯åŒ¹é…
legal_config = {
    "vector_similarity_weight": 0.3,
    "full_text_weight": 0.7
}
```

---

### 10.4 ç›‘æ§ä¸ç»´æŠ¤

#### 1. å®šæœŸæ£€æŸ¥

```python
# æ£€æŸ¥è§£æå¤±è´¥çš„æ–‡æ¡£
def check_failed_documents():
    """æ£€æŸ¥å¹¶é‡è¯•å¤±è´¥æ–‡æ¡£"""
    failed_docs = Document.select().where(
        Document.run == TaskStatus.FAIL.value
    )
    
    for doc in failed_docs:
        logging.warning(f"Failed document: {doc.id} - {doc.progress_msg}")
        
        # é‡è¯•è§£æ
        retry_parse(doc.id)
```

#### 2. ç»Ÿè®¡åˆ†æ

```python
# çŸ¥è¯†åº“ç»Ÿè®¡
def kb_statistics(kb_id):
    """ç”ŸæˆçŸ¥è¯†åº“ç»Ÿè®¡æŠ¥å‘Š"""
    kb = KnowledgebaseService.get_by_id(kb_id)
    
    stats = {
        "æ–‡æ¡£æ€»æ•°": kb.doc_num,
        "åˆ†å—æ€»æ•°": kb.chunk_num,
        "Tokenæ€»æ•°": kb.token_num,
        "å¹³å‡åˆ†å—æ•°": kb.chunk_num / kb.doc_num if kb.doc_num > 0 else 0,
        "è§£ææˆåŠŸç‡": calculate_success_rate(kb_id),
        "å­˜å‚¨å ç”¨": calculate_storage_size(kb_id)
    }
    
    return stats
```

#### 3. æ¸…ç†ä¼˜åŒ–

```python
# å®šæœŸæ¸…ç†
def cleanup_kb(kb_id):
    """æ¸…ç†çŸ¥è¯†åº“"""
    # 1. åˆ é™¤å¤±è´¥çš„æ–‡æ¡£
    failed_docs = Document.select().where(
        Document.kb_id == kb_id,
        Document.run == TaskStatus.FAIL.value,
        Document.create_time < datetime.now() - timedelta(days=7)
    )
    for doc in failed_docs:
        DocumentService.remove_document(doc)
    
    # 2. æ¸…ç†å­¤ç«‹çš„åˆ†å—
    cleanup_orphan_chunks(kb_id)
    
    # 3. ä¼˜åŒ–ç´¢å¼•
    optimize_es_index(kb_id)
```

---

### ç›¸å…³æ–‡æ¡£

- [çŸ¥è¯†åº“æ¨¡å— README](./README.md)
- [åˆ›å»ºçŸ¥è¯†åº“æ—¶åºå›¾](./01-create-kb-sequence.puml)
- [æ–‡æ¡£ä¸Šä¼ æ—¶åºå›¾](./02-upload-document-sequence.puml)
- [æ–‡æ¡£è§£ææ—¶åºå›¾](./03-parse-document-sequence.puml)
- [å‘é‡æ£€ç´¢æ—¶åºå›¾](./04-vector-search-sequence.puml)
- [çŸ¥è¯†åº“æ›´æ–°æ—¶åºå›¾](./05-update-kb-sequence.puml)
- [çŸ¥è¯†åº“åˆ é™¤æ—¶åºå›¾](./06-delete-kb-sequence.puml)

