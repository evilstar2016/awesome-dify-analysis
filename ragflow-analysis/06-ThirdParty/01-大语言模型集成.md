# 大语言模型（LLM）集成

## 概述

RAGFlow 支持 20+ 大语言模型提供商，通过统一的抽象接口实现了对不同 LLM 服务的无缝集成。项目采用**工厂模式**和**适配器模式**，允许用户根据需求灵活选择和切换不同的模型提供商。

## 架构设计

### 核心接口

所有 LLM 提供商都实现了 `Base` 基类定义的统一接口：

```python
class Base:
    """LLM 基类接口"""
    
    def chat(self, 
             system: str,
             history: list[dict],
             gen_conf: dict) -> str:
        """同步聊天接口"""
        raise NotImplementedError
        
    def chat_streamly(self,
                      system: str, 
                      history: list[dict],
                      gen_conf: dict) -> Iterator[str]:
        """流式聊天接口"""
        raise NotImplementedError
        
    @property
    def default_model(self) -> str:
        """默认模型名称"""
        raise NotImplementedError
```

**代码位置**：[rag/llm/chat_model.py](../../rag/llm/chat_model.py)

### 工厂配置

LLM 提供商配置存储在 `conf/llm_factories.json`：

```json
{
  "factory_llm_infos": [
    {
      "name": "OpenAI",
      "tags": "LLM,TEXT EMBEDDING,TTS,TEXT RE-RANK,SPEECH2TEXT",
      "llm": [
        {
          "llm_name": "gpt-4o",
          "tags": "LLM,CHAT,128K,IMAGE2TEXT",
          "max_tokens": 128000,
          "model_type": "chat",
          "is_tools": true
        }
      ]
    }
  ]
}
```

**配置文件**：[conf/llm_factories.json](../../conf/llm_factories.json)

## 支持的提供商

### 1. OpenAI 系列

#### 支持的模型

| 模型名称 | 上下文长度 | 功能标签 | 工具调用 |
|---------|----------|----------|---------|
| gpt-5.2-pro | 400K | LLM, CHAT, IMAGE2TEXT | ✅ |
| gpt-5.2 | 400K | LLM, CHAT, IMAGE2TEXT | ✅ |
| gpt-5.1 | 400K | LLM, CHAT, IMAGE2TEXT | ✅ |
| gpt-5 | 400K | LLM, CHAT, IMAGE2TEXT | ✅ |
| gpt-4.1 | 1M | LLM, CHAT, IMAGE2TEXT | ✅ |
| gpt-4o | 128K | LLM, CHAT, IMAGE2TEXT | ✅ |
| gpt-4o-mini | 128K | LLM, CHAT, IMAGE2TEXT | ✅ |
| o3 | 200K | LLM, CHAT, IMAGE2TEXT | ✅ |
| gpt-3.5-turbo | 4K | LLM, CHAT | ❌ |

#### 配置示例

```yaml
# 环境变量
OPENAI_API_KEY=sk-xxxxxxxxxxxxx
OPENAI_BASE_URL=https://api.openai.com/v1  # 可选

# 服务配置
llm:
  factory: OpenAI
  model_name: gpt-4o
  temperature: 0.7
  max_tokens: 2000
```

#### 代码实现

```python
from openai import OpenAI

class OpenAI_APIChat(Base):
    _FACTORY_NAME = "OpenAI"
    
    def __init__(self, key, model_name, **kwargs):
        self.client = OpenAI(api_key=key, base_url=kwargs.get("base_url"))
        self.model_name = model_name
```

**实现类**：`OpenAI_APIChat` ([rag/llm/chat_model.py](../../rag/llm/chat_model.py#L734))

### 2. Anthropic Claude

#### 支持的模型

| 模型名称 | 上下文长度 | 功能特性 |
|---------|----------|----------|
| claude-3-opus-20240229 | 200K | 最强推理能力 |
| claude-3-5-sonnet-20241022 | 200K | 平衡性能 |
| claude-3-5-haiku-20241022 | 200K | 快速响应 |
| claude-3-haiku-20240307 | 200K | 经济实惠 |

#### 配置示例

```yaml
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxx

llm:
  factory: Anthropic
  model_name: claude-3-5-sonnet-20241022
  temperature: 0.7
```

#### 依赖包

```toml
anthropic==0.34.1
```

**实现类**：通过 `LiteLLMBase` 统一调用

### 3. Google Gemini

#### 支持的模型

- `gemini-2.0-pro`
- `gemini-1.5-pro`
- `gemini-1.5-flash`
- `gemini-2.5-flash`
- `gemini-exp-1206`

#### 配置示例

```yaml
GOOGLE_API_KEY=xxxxxxxxxxxxx

llm:
  factory: Google
  model_name: gemini-2.0-pro
```

#### 依赖包

```toml
google-genai>=1.41.0
google-generativeai>=0.8.1
```

**实现类**：`GoogleChat` ([rag/llm/chat_model.py](../../rag/llm/chat_model.py))

### 4. 国内大模型

#### 4.1 通义千问（QWen）

```yaml
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxx

llm:
  factory: Tongyi-Qianwen
  model_name: qwen-max
```

**依赖**：`dashscope==1.20.11`

#### 4.2 智谱 AI（ZhipuAI）

```yaml
ZHIPUAI_API_KEY=xxxxxxxxxxxxx

llm:
  factory: ZHIPU-AI
  model_name: glm-4
```

**依赖**：`zhipuai==2.0.1`

#### 4.3 百度文心（Baidu Yiyan）

```yaml
BAIDU_API_KEY=xxxxxxxxxxxxx
BAIDU_SECRET_KEY=xxxxxxxxxxxxx

llm:
  factory: ERNIE-Bot
  model_name: ernie-4.0
```

**依赖**：`qianfan==0.4.6`

#### 4.4 百川智能（BaiChuan）

```yaml
BAICHUAN_API_KEY=xxxxxxxxxxxxx

llm:
  factory: BaiChuan
  model_name: Baichuan4
```

**实现类**：`BaiChuanChat`

#### 4.5 火山引擎（VolcEngine）

```yaml
VOLC_ACCESS_KEY=xxxxxxxxxxxxx
VOLC_SECRET_KEY=xxxxxxxxxxxxx

llm:
  factory: Volcano
  model_name: doubao-pro-128k
```

**依赖**：`volcengine==1.0.194`
**实现类**：`VolcEngineChat`

#### 4.6 讯飞星火（Spark）

```yaml
SPARK_APP_ID=xxxxxxxxxxxxx
SPARK_API_KEY=xxxxxxxxxxxxx
SPARK_API_SECRET=xxxxxxxxxxxxx

llm:
  factory: Spark
  model_name: spark-v3.5
```

**实现类**：`SparkChat`

#### 4.7 腾讯混元（Hunyuan）

```yaml
TENCENT_SECRET_ID=xxxxxxxxxxxxx
TENCENT_SECRET_KEY=xxxxxxxxxxxxx

llm:
  factory: Tencent
  model_name: hunyuan-lite
```

**依赖**：`tencentcloud-sdk-python==3.0.1478`
**实现类**：`HunyuanChat`

### 5. 开源/自托管模型

#### 5.1 Ollama

本地运行开源模型：

```yaml
OLLAMA_HOST=http://localhost:11434

llm:
  factory: Ollama
  model_name: llama3.2
```

**依赖**：`ollama>=0.5.0`
**实现类**：基于 `OpenAI_APIChat`（兼容 OpenAI API）

#### 5.2 LocalAI

自托管的 OpenAI 替代方案：

```yaml
LOCAL_AI_BASE_URL=http://localhost:8080

llm:
  factory: LocalAI
  model_name: your-model
```

**实现类**：`LocalAIChat`

#### 5.3 Xinference

分布式推理框架：

```yaml
XINFERENCE_BASE_URL=http://localhost:9997

llm:
  factory: Xinference
  model_name: qwen-chat
```

**实现类**：`XinferenceChat`

#### 5.4 HuggingFace

直接使用 HuggingFace 模型：

```yaml
HUGGINGFACE_API_KEY=hf_xxxxxxxxxxxxx

llm:
  factory: HuggingFace
  model_name: meta-llama/Llama-3.2-3B
```

**实现类**：`HuggingFaceChat`

#### 5.5 ModelScope

魔搭社区模型：

```yaml
MODELSCOPE_API_KEY=xxxxxxxxxxxxx

llm:
  factory: ModelScope
  model_name: qwen-turbo
```

**实现类**：`ModelScopeChat`

### 6. 其他商业提供商

#### 6.1 Cohere

```yaml
COHERE_API_KEY=xxxxxxxxxxxxx

llm:
  factory: Cohere
  model_name: command-r-plus
```

**依赖**：`cohere==5.6.2`

#### 6.2 Mistral AI

```yaml
MISTRAL_API_KEY=xxxxxxxxxxxxx

llm:
  factory: Mistral
  model_name: mistral-large-latest
```

**依赖**：`mistralai==0.4.2`
**实现类**：`MistralChat`

#### 6.3 Replicate

```yaml
REPLICATE_API_TOKEN=xxxxxxxxxxxxx

llm:
  factory: Replicate
  model_name: meta/llama-3-70b
```

**依赖**：`replicate==0.31.0`
**实现类**：`ReplicateChat`

#### 6.4 Groq

```yaml
GROQ_API_KEY=gsk_xxxxxxxxxxxxx

llm:
  factory: Groq
  model_name: mixtral-8x7b-32768
```

**依赖**：`groq==0.9.0`

#### 6.5 LeptonAI

```yaml
LEPTON_API_KEY=xxxxxxxxxxxxx

llm:
  factory: LeptonAI
  model_name: llama3.1-70b
```

**实现类**：`LeptonAIChat`

## 高级特性

### 1. 工具调用（Function Calling）

支持工具调用的模型可以调用外部函数：

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "获取天气信息",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                }
            }
        }
    }
]

response = llm.chat(
    messages=[{"role": "user", "content": "北京天气如何？"}],
    tools=tools
)
```

**支持的模型**：GPT-4 系列、Claude 3 系列、Gemini 系列

### 2. 多模态输入（Vision）

支持图像输入的模型：

```python
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "这张图片里有什么？"},
            {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
        ]
    }
]

response = llm.chat(messages=messages)
```

**支持的模型**：
- GPT-4o 系列
- Claude 3 系列
- Gemini 系列

### 3. 流式响应

所有模型都支持流式输出：

```python
for chunk in llm.chat_streamly(messages=messages):
    print(chunk, end='', flush=True)
```

### 4. ReAct 模式

支持 ReAct（Reasoning + Acting）模式的推理：

```python
llm.chat(
    messages=messages,
    gen_conf={
        "react_mode": ReActMode.FULL  # FULL, THINKING_ONLY, DISABLED
    }
)
```

## 错误处理

### 常见错误码

```python
class LLMErrorCode(Enum):
    AUTHENTICATION_ERROR = "认证失败"
    RATE_LIMIT_ERROR = "超出速率限制"
    CONTEXT_LENGTH_ERROR = "上下文长度超限"
    API_CONNECTION_ERROR = "API 连接失败"
    TIMEOUT_ERROR = "请求超时"
    MODEL_NOT_FOUND = "模型不存在"
```

### 错误处理策略

```python
try:
    response = llm.chat(messages=messages)
except RateLimitError:
    # 等待后重试
    time.sleep(60)
    response = llm.chat(messages=messages)
except ContextLengthError:
    # 截断消息
    messages = truncate_messages(messages, max_tokens)
    response = llm.chat(messages=messages)
except AuthenticationError:
    # 检查 API Key
    logger.error("Invalid API key")
```

## 性能优化

### 1. 连接池

```python
from httpx import Client, Limits

http_client = Client(
    limits=Limits(max_connections=100, max_keepalive_connections=20),
    timeout=30.0
)
```

### 2. 批量请求

```python
# 使用异步批量处理
async def batch_chat(messages_list):
    tasks = [llm.chat_async(messages) for messages in messages_list]
    return await asyncio.gather(*tasks)
```

### 3. 缓存策略

```python
# 使用 Redis 缓存相同请求的响应
cache_key = hashlib.md5(json.dumps(messages).encode()).hexdigest()
cached_response = redis_conn.get(cache_key)
if cached_response:
    return cached_response
```

## 使用示例

### 基础对话

```python
from rag.llm import chat_model

# 初始化 LLM
llm = chat_model.OpenAI_APIChat(
    key=os.getenv("OPENAI_API_KEY"),
    model_name="gpt-4o"
)

# 单轮对话
response = llm.chat(
    system="你是一个有帮助的助手",
    history=[
        {"role": "user", "content": "什么是 RAG？"}
    ],
    gen_conf={"temperature": 0.7, "max_tokens": 500}
)

print(response)
```

### 多轮对话

```python
history = []

while True:
    user_input = input("You: ")
    if user_input.lower() == "quit":
        break
    
    history.append({"role": "user", "content": user_input})
    
    response = llm.chat(
        system="你是一个有帮助的助手",
        history=history,
        gen_conf={"temperature": 0.7}
    )
    
    history.append({"role": "assistant", "content": response})
    print(f"Assistant: {response}")
```

### 流式输出

```python
def stream_chat(llm, messages):
    print("Assistant: ", end='')
    full_response = ""
    
    for chunk in llm.chat_streamly(
        system="你是一个有帮助的助手",
        history=messages,
        gen_conf={"temperature": 0.7}
    ):
        print(chunk, end='', flush=True)
        full_response += chunk
    
    print()  # 换行
    return full_response
```

## 监控与日志

```python
import logging

logger = logging.getLogger(__name__)

# 记录 API 调用
logger.info(f"Calling {llm._FACTORY_NAME} API")
logger.debug(f"Model: {llm.model_name}, Messages: {messages}")

# 记录响应时间
import time
start_time = time.time()
response = llm.chat(messages=messages)
elapsed = time.time() - start_time
logger.info(f"Response time: {elapsed:.2f}s")
```

## 成本优化

### 1. 模型选择策略

```python
def select_model(task_complexity):
    if task_complexity == "simple":
        return "gpt-4o-mini"  # 便宜快速
    elif task_complexity == "medium":
        return "gpt-4o"       # 平衡
    else:
        return "gpt-4.1"      # 复杂任务
```

### 2. 提示词优化

- 使用简洁明确的提示词
- 避免不必要的上下文
- 使用 system message 替代重复的 user message

### 3. 响应长度控制

```python
gen_conf = {
    "max_tokens": 500,  # 限制输出长度
    "stop": ["\n\n"]    # 遇到停止词立即结束
}
```

## 安全最佳实践

1. **API Key 管理**
   - 使用环境变量存储
   - 定期轮换密钥
   - 不要在代码中硬编码

2. **内容过滤**
   ```python
   # OpenAI Moderation API
   moderation_result = client.moderations.create(input=user_input)
   if moderation_result.results[0].flagged:
       return "输入包含不当内容"
   ```

3. **速率限制**
   ```python
   from ratelimit import limits, sleep_and_retry
   
   @sleep_and_retry
   @limits(calls=60, period=60)  # 每分钟最多 60 次
   def call_llm_api(messages):
       return llm.chat(messages=messages)
   ```

## 故障排查

### 问题 1：API Key 无效

```bash
# 检查环境变量
echo $OPENAI_API_KEY

# 测试 API 连接
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

### 问题 2：速率限制

```python
# 实现指数退避
import time
from openai import RateLimitError

max_retries = 5
for attempt in range(max_retries):
    try:
        response = llm.chat(messages=messages)
        break
    except RateLimitError:
        if attempt < max_retries - 1:
            wait_time = 2 ** attempt
            time.sleep(wait_time)
```

### 问题 3：上下文长度超限

```python
from tiktoken import encoding_for_model

def count_tokens(messages, model="gpt-4"):
    encoding = encoding_for_model(model)
    num_tokens = 0
    for message in messages:
        num_tokens += len(encoding.encode(message["content"]))
    return num_tokens

# 截断历史消息
def truncate_history(history, max_tokens=120000):
    while count_tokens(history) > max_tokens:
        history.pop(0)  # 移除最早的消息
    return history
```

## 测试

```bash
# 运行 LLM 集成测试
uv run pytest test/test_llm.py -v

# 测试特定提供商
uv run pytest test/test_llm.py::test_openai_chat
```

## 参考文档

- [OpenAI API 文档](https://platform.openai.com/docs)
- [Anthropic API 文档](https://docs.anthropic.com)
- [Google Gemini 文档](https://ai.google.dev/docs)
- [通义千问文档](https://help.aliyun.com/zh/dashscope/)
- [智谱 AI 文档](https://open.bigmodel.cn/dev/api)

## 贡献指南

添加新的 LLM 提供商：

1. 在 `rag/llm/chat_model.py` 创建新类
2. 继承 `Base` 基类
3. 实现必需方法
4. 在 `conf/llm_factories.json` 添加配置
5. 添加测试用例
6. 更新文档

## 许可证

遵循项目主许可证 [Apache 2.0](../../LICENSE)
